{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Mathematical Theory of Data Assimilation with Applications:<br>\n",
    "\n",
    "<p class=\"fragment\">Tutorial part 1 of 4 --- Data assimilation in a Bayesian perspective<p></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> Introduction and outline</h3>\n",
    "<ul>\n",
    "<li class=\"fragment\"> The purpose of these tutorials is to introduce the basic mathematical and statistical ideas that underpins data assimilation methods.</li>\n",
    "  <li class=\"fragment\"> We will not belabor the theortical details nor introduce state-of-the-art methods; </li>\n",
    "      <ul>\n",
    "          <li class=\"fragment\">rather we will favor a mix of mathematical \"intuition\" and learning by example in simple problems.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\"> To follow along in the exercises, you should have the following:</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\"> An up-to-date installation of Python version 3, with Jupyter notebooks and standard scientific python libraries (Scipy, Numpy, Matplotlib) installed;</li>\n",
    "        <li class=\"fragment\"> a pen and paper;</li>\n",
    "        <li class=\"fragment\"> an internet connection;</li>\n",
    "        <li class=\"fragment\"> and a background in multivariate calculus, linear algebra, statistics and elementary numerical methods.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Bayesian inference in physical systems</h3>\n",
    "\n",
    "<ul>\n",
    "<li class=\"fragment\"> \"Data assimilation\" is a term for a broad umbrella of methods in computational statistics and optimization;</li> \n",
    "<li class=\"fragment\"> these methods are distinguished by the objective of combining:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\"> inherently uncertain and incomplete observational data; with </li>\n",
    "        <li class=\"fragment\"> the state of a (typically chaotic) <em>physics-based, numerical model</em>.</li>\n",
    "    </ol>\n",
    "<li class=\"fragment\"> As such, an understanding of DA benefits from an interdiscplinary background in statistics, applied maths and physics.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> Bayesian inference in physical systems continued</h3>\n",
    "<ul>\n",
    "<li class=\"fragment\"> Theoretically, this is often framed (and unified) as a <em>Bayesian inference</em> problem, </li>\n",
    "  <ul>\n",
    "  <li class=\"fragment\"> the goal in this framework is to estimate the posterior distribution for the model state or some statistic of it.</li>\n",
    "    </ul>\n",
    "<li class=\"fragment\"> However, due to \n",
    "    <ol>\n",
    "        <li class=\"fragment\"> the computational challenge of the extremely high dimensionality of models and observations; </li>\n",
    "        <li class=\"fragment\"> and the need to perform analyses in real time;</li> \n",
    "    </ol>\n",
    "    <li class=\"fragment\">most techniques focus on only approximating the first two moments of the posterior, and/or its mode.</li>    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Historical foundations</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">Two dominant paradigms have emerged for the solution to DA problems: (i) statistical; and (ii) variational.</li>\n",
    "  <li class=\"fragment\"> In its simplest statistical form, data assimilation can take a formulation as a kind of multiple regression.</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">However, due to the time-dependence of the distribution in the numerical model, this type of regression will look quite different from purely data-based learning.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\">Secondly, a wide class of techniques emerge from the calculus of variations.</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">These techniques were developed from optimal control in PDEs, by minimizing cost-functions explicitly based on the model.</li>\n",
    "    </ul>\n",
    "     <li class=\"fragment\">Both DA paradigms have been merged in a Bayesian framework and many DA methods use a mixture of these techniques.</li> \n",
    "    <ul>\n",
    "        <li class=\"fragment\">Indeed, much of the state-of-the-art operationally uses a hybrid-ensemble-variational approach.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"fragment\">The minimum variance solution</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> Suppose we have two <em>independent</em> observations $T_1, T_2$ of an unknown scalar quantity, the air temperature at the surface of Reading, defined as $T_t$.</li>\n",
    "  <li class=\"fragment\"> We assume (for the moment) that the temperature is deterministic (and unkown), but the observations will be random, i.e.,\n",
    "      $\\begin{align}\n",
    "      T_1 &= T_t + \\epsilon_1 \\\\\n",
    "      T_2 &= T_t + \\epsilon_2\n",
    "      \\end{align}$\n",
    "      </li>\n",
    "   <li class=\"fragment\"> We will assume furthermore that \n",
    "    $\\begin{align}\n",
    "    \\epsilon_1 &\\sim N(0, \\sigma_1^2)\\\\\n",
    "    \\epsilon_2 &\\sim N(0, \\sigma_2^2)\n",
    "    \\end{align}$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Observational errors</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> The values $\\epsilon_1, \\epsilon_2$ are known as the observational errors.</li>\n",
    "    <li class=\"fragment\"> By their definitions above, we have assumed that the observations are unbiased and uncorrelated.</li>\n",
    "    <li class=\"fragment\"> <b>Q:</b> if the expectation is denoted $\\mathbb{E}$, how do we write that the observations are unbiased and uncorrelated mathematically? </li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">To say the observations are unbiased $\\Leftrightarrow \\mathbb{E}[T_i ] = T_t$ </li>\n",
    "        <li class=\"fragment\">To say the observations are uncorrelated $\\Leftrightarrow \\mathbb{E}[\\epsilon_1 \\epsilon_2] = 0$\n",
    "    </ul>\n",
    "    <li class=\"fragment\"> The former is implied because the errors of the observations are mean zero, while the later is implied by (but not equivalent to) the independence of the observations.</li>\n",
    "</ul>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Observational errors continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> The meaning of the expectation in the above is to say, </li>\n",
    "        <ol>\n",
    "        <li class=\"fragment\">if we were able to make many independent replicates of the observations, in their average we would obtain the true temperature; and\n",
    "        <li class=\"fragment\">we can't obtain any information on the error in measurement 1 from the value of measurement 2 or vice versa.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Estimating the temperature</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> Let's suppose we want to estimate the true temperature by a linear combination of the two measurements. </li>\n",
    "    <li class=\"fragment\"> That is, we will define an \"analyzed\" temperature as $T_a$ where,\n",
    "    \\begin{align}\n",
    "    T_a \\triangleq a_1 T_1 + a_2 T_2\n",
    "    \\end{align}\n",
    "   </li>\n",
    "   \n",
    "   <li class=\"fragment\"> We will require that the analysis is unbiased, i.e.,\n",
    "    \\begin{align}\n",
    "    \\mathbb{E}[T_a] = T_t &\\Leftrightarrow a_1 + a_2 = 1\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"><b>Q:</b> why is \n",
    "    \\begin{align}\n",
    "    \\mathbb{E}[T_a] = T_t \\Leftrightarrow a_1 + a_2 = 1?\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"><b>A:</b> with $\\mathbb{E}[\\epsilon_i]=0$, we see that $\\mathbb{E}[T_a] = (a_1 + a_2)T_t$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Estimating the temperature continued</h3>\n",
    "<ul>    \n",
    "   <li class=\"fragment\"> We will also choose $a_1$ and $a_2$ in order to minimize the mean-square-error of the analysis, defined as\n",
    "    \\begin{align}\n",
    "       \\sigma_a^2 &= \\mathbb{E}\\left[\\left(T_a - T_t\\right)^2\\right] \\\\\n",
    "       &=\\mathbb{E}\\left[ \\left( a_1\\left\\{T_1 - T_t\\right\\} + a_2\\left\\{T_2 - T_t\\right\\}\\right)^2 \\right]\n",
    "    \\end{align}\n",
    "   </li>\n",
    "    <li class=\"fragment\"> Substituting the relationship $a_2 = 1 - a_1$, we can compute the derivative of the variance of the analysis solution above with respect to $a_1$.</li>\n",
    "    <li class=\"fragment\"> We can thus derive,\n",
    "        \\begin{align}\n",
    "        a_1 = \\frac{\\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2} & & a_2 = \\frac{\\sigma_1^2}{\\sigma_1^2 + \\sigma_2^2} \n",
    "        \\end{align}\n",
    "       as the choice of weights that minimizes the analysis variance.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Optimal weights</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Exercise (3 minutes):</b> using $a_2 = 1- a_1$, derive the minimizer of\n",
    "    \\begin{align}\n",
    "    \\sigma_a^2 =\\mathbb{E}\\left[ \\left( a_1\\left\\{T_1 - T_t\\right\\} + a_2\\left\\{T_2 - T_t\\right\\}\\right)^2 \\right]\n",
    "    \\end{align}\n",
    "    with respect to $a_1$.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Optimal weights continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Solution:</b>\n",
    "    \\begin{align}\n",
    "    \\sigma^2_a &= \\mathbb{E}\\left[\\left(a_1 \\epsilon_1 + \\left\\{1-a_1\\right\\}\\epsilon_2\\right)^2\\right] \\\\\n",
    "        & =a_1^2 \\sigma_1^2 + \\left(1 - a_1\\right)^2 \\sigma_2^2\n",
    "    \\end{align}\n",
    "    because $\\epsilon_1$ and $\\epsilon_2$ are uncorrelated.</li>\n",
    "    <li class=\"fragment\"> Therefore, setting the derivative of $\\partial_{a_1} \\sigma_a^2 = 0$ we recover,\n",
    "    \\begin{align}\n",
    "     & 0= 2 a_1\\sigma_1^2 -  2 \\sigma_2^2 + 2 a_1 \\sigma_2^2 \\\\\n",
    "    \\Leftrightarrow & a_1 = \\frac{\\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">The value for $a_2$ can be derived symmetrically in the index.</li>\n",
    "</ul>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Optimal weights continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">Notice that \n",
    "        \\begin{align}\n",
    "        a_1 = \\frac{\\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2} & & \\Leftrightarrow & & a_1 = \\frac{\\frac{1}{\\sigma_1^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}\n",
    "        \\end{align}</li>\n",
    "    <li class=\"fragment\">The inverse of the variance of the observation is known as the <em>precision</em> of the observation.</li>\n",
    "    <li class=\"fragment\">This tells us that we weight the observations in the optimal solution proportionately to their precision.</li> \n",
    "        <li class=\"fragment\">Equivalently, we can say we weight the observations inverse-proportionately to their uncertainty.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Optimal state by cost function approach</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> We can also obtain the same solution for the \"best\" analysis $T_a$ by minimizing a cost function. \n",
    "    </li>\n",
    "    <li class=\"fragment\"> The cost function measures the squared difference of an arbitrary temperature $T$ from the two observations, weighted inverse-proportionately to their uncertainty.\n",
    "    </li>\n",
    "    <li class=\"fragment\">The resulting cost function is given by $J(T)$,\n",
    "    \\begin{align}\n",
    "    J(T) \\triangleq \\frac{1}{2} \\left[ \\frac{\\left(T_1 - T\\right)^2}{\\sigma_1^2} + \\frac{\\left(T_2 - T\\right)^2}{\\sigma_2^2} \\right].\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> Under the earlier Gaussian error assumptions, minimizing this cost function corresponds to the maximum likelihood estimate/ maximum a posterior state.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Optimal state by cost function approach continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Q:</b> how can we show that the solution \n",
    "        \\begin{align}\n",
    "        T_a \\triangleq \\frac{T_1 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2} + \\frac{T_2 \\sigma_1^2}{\\sigma_1^2 + \\sigma_2^2}\n",
    "        \\end{align}\n",
    "        is the minimizer to the cost function\n",
    "    \\begin{align}\n",
    "    J(T) \\triangleq \\frac{1}{2} \\left[ \\frac{\\left(T_1 - T\\right)^2}{\\sigma_1^2} + \\frac{\\left(T_2 - T\\right)^2}{\\sigma_2^2} \\right]?\n",
    "    \\end{align}\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Optimal state by cost function approach continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>A:</b> we note that\n",
    "    \\begin{align}\n",
    "    \\partial_T J(T)=  -\\frac{\\left(T_1 - T\\right)}{\\sigma_1^2} - \\frac{\\left(T_2 - T\\right)}{\\sigma_2^2}.\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">Therefore,\n",
    "    \\begin{align}\n",
    "    \\partial_TJ\\vert_{T_a}&=  -\\frac{\\left(T_1 - \\frac{T_1 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2} - \\frac{T_2 \\sigma_1^2}{\\sigma_1^2 + \\sigma_2^2}\\right)}{\\sigma_1^2} - \\frac{\\left(T_2 - \\frac{T_1 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2} - \\frac{T_2 \\sigma_1^2}{\\sigma_1^2 + \\sigma_2^2}\\right)}{\\sigma_2^2}\\\\\n",
    "        &= \\frac{ \\frac{T_1 \\sigma_2^2}{\\sigma_1^2}  + T_2 + T_1 + \\frac{T_2 \\sigma_1^2}{\\sigma_2^2} - \\frac{T_1 \\sigma_2^2}{\\sigma_1^2} - T_1 - T_2 - \\frac{T_2 \\sigma_1^2}{\\sigma_2^2} }{\\sigma_1^2 + \\sigma_2^2} = 0\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> The cost function is positive and quadratic, so therefore this the unique, global minimum.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Maximum likelihood</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> The probability distribution of an observation $T_i$ given the true value $T$ and the standard deviation $\\sigma_i$ is given as\n",
    "    \\begin{align}\n",
    "    P_{\\sigma_i}(T_i \\vert T) = \\frac{1}{\\sqrt{2\\pi}\\sigma_i}e^{-\\frac{\\left(T_i - T\\right)^2}{2\\sigma_i^2}} \n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">This corresponds then to saying the likelihood of the true value $T$ given the observation $T_i$ is given as $L(T\\vert T_i, \\sigma_i) = P_{\\sigma_i}\\left(T_i \\vert T\\right)$.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Maximum likelihood continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> <b>Q:</b> what is the most likely value for the temperature given the two observations $T_1$ and $T_2$?</li> \n",
    "    <li class=\"fragment\"> <b>A:</b> the most likely value for the true temperature $T$ is the one that maximizes the joint probability, i.e.,\n",
    "    \\begin{align}\n",
    "    T_a &\\triangleq \\max_T L\\left( T \\vert T_1, T_2,\\sigma_1, \\sigma_2\\right) \\\\\n",
    "        &= \\max_T P_{\\sigma_1}(T_1 \\vert T)P_{\\sigma_2}(T_2 \\vert T) \\\\ \n",
    "        &= \\max_T \\frac{1}{\\sqrt{2\\pi}\\sigma_1}e^{-\\frac{\\left(T_1 - T\\right)^2}{2\\sigma_1^2}}\n",
    "         \\frac{1}{\\sqrt{2\\pi}\\sigma_2}e^{-\\frac{\\left(T_2 - T\\right)^2}{2\\sigma_2^2}}\\\\\n",
    "        &= \\max_T \\frac{1}{2\\pi\\sigma_1 \\sigma_2}e^{-\\frac{\\left(T_1 - T\\right)^2}{2\\sigma_1^2} -\\frac{\\left(T_2 - T\\right)^2}{2\\sigma_2^2}}\n",
    "    \\end{align}\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Log-likelihood</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">Log is a monotinic function, so that the maximum likelihood is also the maximum log-likelihood.\n",
    "    </li>\n",
    "    <li class=\"fragment\">That is,\n",
    "        \\begin{align}\n",
    "        T_a &=  \\max_T \\log L\\left( T \\vert T_1, T_2,\\sigma_1, \\sigma_2\\right) \\\\\n",
    "        & = \\max_T \\left[\\mathrm{constant } - \\frac{\\left(T_1 - T\\right)^2}{2\\sigma_1^2} - \\frac{\\left(T_2 - T\\right)^2}{2\\sigma_2^2} \\right]\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> It is an extremely useful property that the minimum variance analysis solution is equal to the maximum likelihood analysis solution.</li>\n",
    "    <li class=\"fragment\">However, this is not true in general and this equivalence is based on the Gaussianity of the error distributions;</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">generally, solving for one or the other in a real geophysical system will produce different methods with differing results.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Bayesian perspective</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">All of the above derivations are generic in frequentist statistics and don't essentially relate to the unique issue of data assimilation: the <em>physics-based, numerical model</em>.</li>\n",
    "    <li class=\"fragment\">In a realistic data assimilation problem, we will suppose that $T_1$ is actually produced by the physics-based model as a numerical forecast.</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">Because we have generated $T_1$, we will suppose we have a <em>prior</em> distribution for the possible values of $T$ based on $T_1$ as well.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\">We then suppose that $T_2$ represents observational information that is extrinsic from the numerical model, with its own uncertainty.</li>\n",
    "    <li class=\"fragment\"> In this context, it is natural to consider the problem in a Bayesian framework. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Bayesian perspective continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">In the Bayesian perspective, the true temperature $T_t$ is itself is a random variable.</li>\n",
    "    <li class=\"fragment\"> This change of perspective can be viewed by an equivalent statement in our earlier assumptions.</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">Specifically, if\n",
    "        \\begin{align}\n",
    "          T_1 &= T_t + \\epsilon_1; \\\\\n",
    "          T_2 &= T_t + \\epsilon_2;\n",
    "          \\end{align}\n",
    "          </li>\n",
    "   <li class=\"fragment\"> and \n",
    "    \\begin{align}\n",
    "    \\epsilon_1 &\\sim N(0, \\sigma_1^2);\\\\\n",
    "    \\epsilon_2 &\\sim N(0, \\sigma_2^2);\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">we can then view instead that\n",
    "        \\begin{align}\n",
    "        T_t & \\sim N(T_1, \\sigma_1^2) \\\\\n",
    "        T_2 & \\sim N(T_t, \\sigma_2^2)\n",
    "        \\end{align}\n",
    "        </li>\n",
    " </ul>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Bayesian perspective continued</h3>\n",
    "<ul>  \n",
    "      <li class=\"fragment\">With the view that\n",
    "        \\begin{align}\n",
    "        T_t & \\sim N(T_1, \\sigma_1^2) \\\\\n",
    "        T_2 & \\sim N(T_t, \\sigma_2^2)\n",
    "        \\end{align}\n",
    "        <ul>\n",
    "        <li class=\"fragment\"> $T_1$ represents the mean of the prior from which the random variable $T_t$ is drawn.</li>\n",
    "        <li class=\"fragment\">$T_2$ in this case represents a random draw from the normal with mean $T_t$ <em>at the point the outcome has transpired</em>, i.e., when $T_t$ is no longer random and the outcome is available but unkown.</li>\n",
    "         <li class=\"fragment\">The goal then is to find the posterior distribution of model states conditioned on the new observation.</li>\n",
    "            <li class=\"fragment\">This posterior can be used thus to evaluate the likelihood of various model states given all previous numerical forecast and observational data.</li> \n",
    "          </ul>\n",
    "    </ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Bayesian perspective continued</h3>\n",
    "<ul>     \n",
    "<li class=\"fragment\"> This perspective on the learning problem is in many ways philosophically preferable to a classical inverse problem formulation,</li>\n",
    "    <ul>\n",
    "            <li class=\"fragment\">this is particularly because we can evaluate the likelihood of imperfect and fundamentally incorrect model states in their ability to produce a time series close to observed states.</li> \n",
    "    </ul>\n",
    "    <li class=\"fragment\">We do not assume that the atmosphere actually lives in our model, which would be nonsense.</li>\n",
    "</ul>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> Data assimilation in the Bayesian perspective</h3>    \n",
    "<ul>\n",
    "    <li class=\"fragment\">Quintessentially in DA, the prior for the true temperature $P_{T_1,\\sigma_1}(T)$ is defined in relation a numerical model prediction, $T_1$.</li>\n",
    "    <li class=\"fragment\">Then, an observation of the true temperature is supplied with an associated likelihood $L(T \\vert T_2, \\sigma_2)$.</li>\n",
    "    <li class=\"fragment\">We can (theoretically) compute the probability of the observation independent of the forecast, having marginalized over all possible temperatures\n",
    "    \\begin{align}\n",
    "    P_{\\sigma_2}(T_2) = \\int_{T'} \\frac{1}{\\sqrt{2\\pi}\\sigma_i}e^{-\\frac{\\left(T_i - T'\\right)^2}{2\\sigma_i^2}} dT'\n",
    "    \\end{align}\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Data assimilation in the Bayesian perspective continued</h3>\n",
    "<ul>\n",
    "<li class=\"fragment\"> Bayes' law for updating the prior $P(A)$ with the new information on the event $B$ is given as\n",
    "    \\begin{align}\n",
    "    P(A \\vert B) = \\frac{P(B \\vert A) P(A)}{P(B)}\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">We can then write the posterior distribution of the true temperature as\n",
    "    \\begin{align}\n",
    "    P_{\\sigma_2}(T\\vert T_2) \\triangleq \\frac{L_{\\sigma_2}(T_2\\vert  T)P_{T_1,\\sigma_1}(T) }{P_{\\sigma_2}(T_2)}.   \n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> Notice, the denominator doesn't depend on the true state.</li> \n",
    "    <li class=\"fragment\">For this reason, we can neglect this \"normalization\" term in most computational statistical methods;</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">as we will see later, it is usually sufficient to compute the posterior up to proportionality.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\"> Also, notice that the numerator yields the functional form for maximum a posteriori estimate by substitution with the likelihood function.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Bayes' Law</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> Bayes' law is extremely simple theoretically, but until advances in digital computers, it had little pratical value due to the issue with computing analytical solutions with it.</li>\n",
    "    <li class=\"fragment\">Digital computers have allowed practitioners to use \"sampling\" techiques to approximate the rule, which is a theme that we will return to in the final tutorial.</li>\n",
    "     <li class=\"fragment\"> We note, Bayes' law is equivalent to simply re-writing the statement of conditional probabilities,\n",
    "        \\begin{align}\n",
    "        P(A\\cap B) = P(A\\vert B) P(B)\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"><b>Exercise (2 minutes):</b> use the statement of conditional probabilities to derive Bayes' Law.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Bayes' Law continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Solution:</b></li>\n",
    "    <li class=\"fragment\"> By definition, we know\n",
    "        \\begin{align}\n",
    "        P(A \\cap B) &= P(A\\vert B) P(B) \\\\\n",
    "        P(B \\cap A) &= P(B \\vert A) P(A)\\\\\n",
    "        P(A \\cap B) &= P(B \\cap A)\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> Therefore, we find\n",
    "        \\begin{align}\n",
    "        & P(A\\vert B) P(B) = P(B \\vert A) P(A) \\\\\n",
    "        \\Leftrightarrow & P(A\\vert B) = \\frac{P(B \\vert A) P(A)}{P(B)}\n",
    "        \\end{align}\n",
    "        whenever $P(B)\\neq 0$.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Introducing the forecast and observational models</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> Let's formally introduce a simple dynamical model and observational model,\n",
    "        \\begin{align}\n",
    "        \\mathbf{x}_{k} &= \\mathbf{M} \\mathbf{x}_{k-1} & & \\mathbf{M} \\in \\mathbb{R}^{n\\times n} & & \\mathbf{x}_k \\in \\mathbb{R}^n\\\\\n",
    "        \\mathbf{y}_{k} &= \\mathbf{H} \\mathbf{x}_k + \\mathbf{v}_k & & \\mathbf{H} \\in \\mathbb{R}^{d \\times n} & & \\mathbf{y}_k, \\mathbf{v}_k \\in \\mathbb{R}^{d} \n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">Here, the <em>vector</em> $\\mathbf{x}_k$ will correspond to all physical states we study with our model at time $t_k$, e.g., the forecast temperature $T_1$ of earlier --- we suppose that the initial state $\\mathbf{x}_0 \\sim N\\left(\\overline{x}_0, \\mathbf{B}\\right)$.</li>\n",
    "    <li class=\"fragment\"> The <em>matrix</em> $\\mathbf{M}$ defines the time evolution of these states from time $t_{k-1}$ to time $t_{k}$ for all values $k$, corresponding to some numerical model.</li>\n",
    "    <li class=\"fragment\">The <em>vector</em> $\\mathbf{y}_k$ will represent the values of the physical state we observe, e.g., the observed temperature $T_2$ of earlier.</li>\n",
    "    <li class=\"fragment\">The <em>vector</em> $\\mathbf{v}_k \\sim N(0, \\mathbf{R})$ is noise in the observation, corresponding to $\\epsilon_2$ as before.</li>\n",
    "    <li class=\"fragment\">Note that we may include stochasticity in the evolution of the state $\\mathbf{x}_k$, but we neglect this at the moment.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Linear-Gaussian, hidden Markov Model</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> The former model is an example of a linear-Gaussian, hidden Markov model.</li>\n",
    "    <li class=\"fragment\"> In this type of model, we can actually analytically derive the evolution of the Bayesian posterior;</li>\n",
    "    <li class=\"fragment\">however, at the moment, we will consider only the one step update of the \"optimal\" solution found by the cost function.</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>A naive update with fixed prior</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">Let's suppose (naively) that we will always use the same form for the background prior for the true state;</li>\n",
    "    <ul>\n",
    "    <li class=\"fragment\"> that is, let us assume a prior for the true state given as\n",
    "        \\begin{align}\n",
    "        P_{\\mathbf{B},\\mathbf{x}_b}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2}\\vert \\mathbf{B}\\vert}e^{-\\frac{1}{2}\\left(\\mathbf{x} - \\mathbf{x}_b \\right)^\\mathrm{T} \\mathbf{B}^{-1} \\left(\\mathbf{x} - \\mathbf{x}_b \\right)}.\n",
    "        \\end{align}</li>\n",
    "    <li class=\"fragment\">The vector $\\mathbf{x}_b$ will be the forward time evolution of last optimal analysis state $\\mathbf{M}\\mathbf{x}_a$, defining the prior for the true state with the fixed background covariance $\\mathbf{B}$.</li>\n",
    "    <li class=\"fragment\">We then define the likelihood for observation $\\mathbf{y}_k$ depending on the state $\\mathbf{x}$ with \n",
    "        \\begin{align}\n",
    "        L(\\mathbf{x}\\vert \\mathbf{y}_k) = \\frac{1}{(2\\pi)^{d/2}\\vert \\mathbf{R}\\vert}e^{-\\frac{1}{2}\\left( \\mathbf{y}_k - \\mathbf{H}\\mathbf{x} \\right)^\\mathrm{T} \\mathbf{R}^{-1} \\left( \\mathbf{y}_k - \\mathbf{H}\\mathbf{x} \\right)},\n",
    "        \\end{align}\n",
    "        where the likelihood is measured in the observational space $\\mathbb{R}^d$ by transfering the state $\\mathbf{x}$ with $\\mathbf{H}$.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>3D-VAR as naive Bayes</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> With the definitions on the last slide, we can write a naive Bayesian update (with fixed form for the prior) as\n",
    "    \\begin{align}\n",
    "    P_{\\mathbf{x}_b,\\mathbf{B}}(\\mathbf{x} \\vert \\mathbf{y}_k)\\triangleq \\frac{L_\\mathbf{R}(\\mathbf{y}_k \\vert \\mathbf{x} )P_{\\mathbf{x}_b,\\mathbf{B}}(\\mathbf{x}) }{P_{\\mathbf{R}}(\\mathbf{y}_k)} .   \n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> Similar to the one dimensional version, we can recover the (naive) maximum a posteriori state $\\mathbf{x}_a$ by minimizing the cost function\n",
    "    \\begin{align}\n",
    "    J(\\mathbf{x}) &= \\frac{1}{2}\\left[\\left(\\mathbf{x} - \\mathbf{x}_b\\right)^\\mathrm{T} \\mathbf{B}^{-1}\\left(\\mathbf{x} - \\mathbf{x}_b\\right) + \\left(\\mathbf{H}\\mathbf{x} - \\mathbf{y}_k\\right)^\\mathrm{T} \\mathbf{R}^{-1} \\left(\\mathbf{H}\\mathbf{x} - \\mathbf{y}_k\\right)\\right]  \n",
    "        \\end{align}</li>\n",
    "    <li class=\"fragment\"> The solution to the DA problem by minimizing the above cost function is the classical method known as \"3D-VAR\";\n",
    "    <li class=\"fragment\">this stands for the variational solution to the three-physical-state-dimenion, maximum likelihood/ max a posteriori formulation.</li>\n",
    "    <li class=\"fragment\">Once again, each piece of information is weighted inverse-proportionately to the relative uncertainty.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>An example of 3D-VAR</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> We will now consider a numerical example of 3D-VAR.</li>\n",
    "    <li class=\"fragment\"> We will not use a linear model, so the above theoretical considerations will not be strictly accurate;</li> \n",
    "    <ul>\n",
    "        <li class=\"fragment\">however, the historical progression of DA has largely been driven by simplified mathematical assumptions to derive methods, while attempting to apply these methods in complex systems.</li>\n",
    "        <li class=\"fragment\"> We will follow the same progression in these tutorials.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Ikeda Map</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">We will consider the real/ two-dimensional form of the Ikeda map as our model system.</li>\n",
    "    <li class=\"fragment\">This is a \"toy\" model for the complex coordinate evolution of light going across a nonlinear optical resonator.</li>\n",
    "    <li class=\"fragment\"> In complex coordinates, it takes the form,\n",
    "        \\begin{align}\n",
    "        z_{k+1} = A + Bz_k e^{i(\\vert z_k\\vert^2 + C)}.\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> $z_k$ stands for the electrical field inside the resonator at the $k$-th step of rotation in the resonator; </li>\n",
    "    <li class=\"fragment\">$A$ and $C$ are parameters which indicate laser light applied from the outside, and linear phase across the resonator, respectively. </li>\n",
    "    <li class=\"fragment\"> $B\\leq 1$ is called dissipation parameter characterizing the loss of resonator, and in the limit of $B=1$ the Ikeda map becomes a conservative map.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Ikeda map continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> In real, two-dimensional coordinates the map becomes\n",
    "    \\begin{align}\n",
    "    x_{{k+1}}&=1 + u(x_{k}\\cos t_{k}+y_{k}\\sin t_{k})\\\\\n",
    "    y_{{k+1}}&=u(x_{k}\\sin t_{k}+y_{k}\\cos t_{k})\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">Here, $u$ is a parameter and we define\n",
    "    \\begin{align}\n",
    "    t_k = 0.4 - \\frac{6}{1 + x_k^2 + y_k^2}\n",
    "    \\end{align}.\n",
    "    </li>\n",
    "    <li class=\"fragment\">When the parameter $u>0.6$ the map above exhibits a (computationally) simple dynamical system with a chaotic attractor.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Preparing for next time</h3>\n",
    "<ul>\n",
    "        <li class=\"fragment\"> For next time, you should install Python with the standard scientific libraries, such as:</li>\n",
    "            <ol>\n",
    "                <li class=\"fragment\"> Numpy;</li>\n",
    "                <li class=\"fragment\"> Scipy; and</li>\n",
    "                <li class=\"fragment\"> Matplotlib.</li>\n",
    "            </ol>\n",
    "    <li class=\"fragment\"> Additionally, you should install Jupyter and Ipython.</li>\n",
    "    <li class=\"fragment\"> The recommended way is through Anaconda.org, with a full Anaconda installation for a basic suite of libaries or a miniconda installation for a lighter weight package managment.</li>\n",
    "    <li class=\"fragment\">There are installation options for Windows, Mac and Linux systems.</li> \n",
    "    <li class=\"fragment\">We will review this material and begin coding the example next time.</li>\n",
    "      </ul>\n",
    "      \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
