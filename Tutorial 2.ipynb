{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Mathematical Theory of Data Assimilation with Applications:<br>\n",
    "\n",
    "<p class=\"fragment\">Tutorial part 2 of 4 --- 3D-VAR as a naive Bayesian filter<p></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> Jupyter notebooks</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"> In the following, we will utilize Jupyter/ Ipython notebooks to explore a computational example.</li>\n",
    "    <li class=\"fragment\"> These tutorials are made in \"Jupyter notebooks\" which conveniently combine Python (code) with text (markdown). </li>\n",
    "    <li class=\"fragment\"> Notebooks live in the web browser to allow for a modifiable graphical interface for interactive code development, data analysis and visualization.</li> \n",
    "     <li class=\"fragment\"> A notebook consists of \"cells\", which you can work with using your mouse, or more efficiently, your keyboard:\n",
    "         \n",
    "| Navigate                      | Edit              | Exit           | Run                              |\n",
    "| -------------                 | : ------------- : | -------------  | : ------------- :                |\n",
    "| <kbd>↓</kbd> and <kbd>↑</kbd> | <kbd>Enter</kbd>  | <kbd>Esc</kbd> | <kbd>Ctrl</kbd>+<kbd>Enter</kbd> |</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> Jupyter notebooks continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"> When you open a notebook it starts a session of Python in the background. \n",
    "    <li class=\"fragment\"> All of the Python code cells (in a given notebook) are connected -- they use the same Python session and thus share variables, functions, and classes.</li> \n",
    "    <li class=\"fragment\">  For this reason, the order in which you run the cells matters. </li>\n",
    "    <li class=\"fragment\"> We will begin our coding exercises by importing basic scientific libraries.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Pythonic programming</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> Python uses several standard scientific libraries for numerical computing, data processing and visualization.</li>\n",
    "    <li class=\"fragment\"> At the core, there is a Python kernel and interpreter that can take human readable inputs and turn these into machine code.</li>\n",
    "    <li class=\"fragment\"> This is the basic Python functionality, but there are extensive specialized libaries for purpose oriented computing.</li>\n",
    "    <li class=\"fragment\">The most important of these for scientific computing are the following:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">Numpy -- large array manipulation;</li>\n",
    "        <li class=\"fragment\">Scipy -- library of numerical routines and scientific computing ecosystem;</li>\n",
    "        <li class=\"fragment\">Pandas -- R dataframe inspired, data structures and analysis;</li>\n",
    "        <li class=\"fragment\">Scikit-learn -- machine learning libraries;</li>\n",
    "        <li class=\"fragment\">Matplotlib -- Matlab inspired, object oriented plotting library.</li>\n",
    "    </ol>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Pythonic programming continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">To accomodate the flexibility of the Python programming environment, conventions around methods namespaces and scope have been adopted.</li>\n",
    "    <li class=\"fragment\">The convention is to utilize import statements to call methods of the library.</li>\n",
    "    <li class=\"fragment\">For example, we will import the library \"numpy\" as a new object to call methods from</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "    <li class=\"fragment\">The tools we use from numpy will now be called from numpy as an object, with the form of the call looking like \"np.mehtod()\".</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> Numpy arrays</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">Numpy has a method known as \"array\";</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "foo = np.array([[1,2,3], [3,4,5], [5,6,7]])\n",
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Numpy arrays continued</h3>\n",
    "<ul>\n",
    "     <li class=\"fragment\">Arrays function as mathematical multi-linear matricies in arbitrary dimensions.</li>\n",
    "     <li class=\"fragment\">Because arrays are understood as mathematical objects, they have inherent methods for mathematical computation, such as:\n",
    "        <ul>\n",
    "            <li class=\"fragment\">the transpose</li>\n",
    "         </ul>\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "foo.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "</ul>         \n",
    "         <ul>  \n",
    "           <li class=\"fragment\">the dot or matrix product:</li>\n",
    "        </ul>    \n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "foo.dot(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Mathematical functions</h3>\n",
    "   <ul>\n",
    "    <li class=\"fragment\">Mathematical functions also appear as methods in numpy, such as:</li>\n",
    "        <ul>\n",
    "            <li class=\"fragment\">cosine</li>\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.cos(np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul><ul>         \n",
    "         <li class=\"fragment\">sine</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.sin(np.pi/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul><ul>        \n",
    "        <li class=\"fragment\"> natural logarithm</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.log(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Mathematical functions continued</h3>\n",
    "   <ul>\n",
    "   <ul>\n",
    "       <li class=\"fragment\">exponential</li>\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul><ul>\n",
    "       <li class=\"fragment\"> square root </li>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "    <li>elementwise scalar multiplication is given by \"*\", elementwise exponentiation is given by \"**\", and matrix multiplication is given by \"@\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>(Re)-introducing the forecast and observational models</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> Last time we formally introduced a simple dynamical model and observational model,\n",
    "        \\begin{align}\n",
    "        \\mathbf{x}_{k} &= \\mathbf{M} \\mathbf{x}_{k-1} & & \\mathbf{M} \\in \\mathbb{R}^{n\\times n} & & \\mathbf{x}_k \\in \\mathbb{R}^n\\\\\n",
    "        \\mathbf{y}_{k} &= \\mathbf{H} \\mathbf{x}_k + \\mathbf{v}_k & & \\mathbf{H} \\in \\mathbb{R}^{d \\times n} & & \\mathbf{y}_k, \\mathbf{v}_k \\in \\mathbb{R}^{d} \n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">Here, the <em>vector</em> $\\mathbf{x}_k$ corresponds to all physical states we study with our model at time $t_k$ --- we suppose that the initial state $\\mathbf{x}_0 \\sim N\\left(\\overline{x}_0, \\mathbf{B}\\right)$.</li>\n",
    "    <li class=\"fragment\"> The <em>matrix</em> $\\mathbf{M}$ defines the time evolution of these states from time $t_{k-1}$ to time $t_{k}$ for all values $k$, corresponding to some numerical model.</li>\n",
    "    <li class=\"fragment\">The <em>vector</em> $\\mathbf{y}_k$ represents the values of the physical state we observe.</li>\n",
    "    <li class=\"fragment\">The <em>vector</em> $\\mathbf{v}_k \\sim N(0, \\mathbf{R})$ is noise in the observation.</li>\n",
    "    <li class=\"fragment\">Note that we may include stochasticity in the evolution of the state $\\mathbf{x}_k$, but we neglect this at the moment.</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>A naive update with fixed prior</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">Let's suppose (naively) that we will always use the same form for the background prior for the true state;</li>\n",
    "    <ul>\n",
    "    <li class=\"fragment\"> that is, let us assume a prior for the true state given as\n",
    "        \\begin{align}\n",
    "        P_{\\mathbf{B},\\mathbf{x}_b}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2}\\vert \\mathbf{B}\\vert}e^{-\\frac{1}{2}\\left(\\mathbf{x} - \\mathbf{x}_b \\right)^\\mathrm{T} \\mathbf{B}^{-1} \\left(\\mathbf{x} - \\mathbf{x}_b \\right)}.\n",
    "        \\end{align}</li>\n",
    "    <li class=\"fragment\">The vector $\\mathbf{x}_b$ will be the forward time evolution of last \"optimal\" analysis state $\\mathbf{M}\\mathbf{x}_a$, defining the prior for the true state with the fixed background covariance $\\mathbf{B}$.</li>\n",
    "    <li class=\"fragment\">We then define the of the observation $\\mathbf{y}_k$ depending on the state $\\mathbf{x}$ as before with \n",
    "        \\begin{align}\n",
    "        L_\\mathbf{R}( \\mathbf{y}_k \\vert \\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2}\\vert \\mathbf{R}\\vert}e^{-\\frac{1}{2}\\left( \\mathbf{y}_k - \\mathbf{H}\\mathbf{x} \\right)^\\mathrm{T} \\mathbf{R}^{-1} \\left( \\mathbf{y}_k - \\mathbf{H}\\mathbf{x} \\right)},\n",
    "        \\end{align}\n",
    "        where the likelihood is measured in the observational space $\\mathbb{R}^d$ by transfering the state $\\mathbf{x}$ with $\\mathbf{H}$.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>3D-VAR as a naive Bayesian filter</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> With the definitions on the last slide, we can write a naive Bayesian update (with fixed form for the prior) as\n",
    "    \\begin{align}\n",
    "    P_{\\mathbf{x}_b, \\mathbf{B}}(\\mathbf{x}\\vert \\mathbf{y}_k )\\triangleq \\frac{L_\\mathbf{R}(\\mathbf{y}_k\\vert \\mathbf{x})P_{\\mathbf{x}_b,\\mathbf{B}}(\\mathbf{x}) }{P_{\\mathbf{R}}(\\mathbf{y}_k)} .   \n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> Similar to the one dimensional version, we can recover the (naive) maximum a posteriori state $\\mathbf{x}_a$ by minimizing the cost function\n",
    "    \\begin{align}\n",
    "    J(\\mathbf{x}) &= \\frac{1}{2}\\left[\\left(\\mathbf{x} - \\mathbf{x}_b\\right)^\\mathrm{T} \\mathbf{B}^{-1}\\left(\\mathbf{x} - \\mathbf{x}_b\\right) + \\left(\\mathbf{H}\\mathbf{x} - \\mathbf{y}_k\\right)^\\mathrm{T} \\mathbf{R}^{-1} \\left(\\mathbf{H}\\mathbf{x} - \\mathbf{y}_k\\right)\\right]  \n",
    "        \\end{align}</li>\n",
    "    <li class=\"fragment\"> The solution to the DA problem by minimizing the above cost function is the classical method known as \"3D-VAR\";\n",
    "    <li class=\"fragment\">this stands for the variational solution to the three-physical-state-dimenion, maximum likelihood/ max a posteriori formulation.</li>\n",
    "    <li class=\"fragment\">Once again, each piece of information is weighted inverse-proportionately to the relative uncertainty.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Ikeda map</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> In real, two-dimensional coordinates the Ikeda map is given\n",
    "    \\begin{align}\n",
    "    x_{{k+1}}&=1 + u(x_{k}\\cos t_{k}+y_{k}\\sin t_{k})\\\\\n",
    "    y_{{k+1}}&=u(x_{k}\\sin t_{k}+y_{k}\\cos t_{k})\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">Here, $u$ is a parameter and we define\n",
    "    \\begin{align}\n",
    "    t_k = 0.4 - \\frac{6}{1 + x_k^2 + y_k^2}.\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">When the parameter $u>0.6$ the map above exhibits a (computationally) simple dynamical system with a chaotic attractor.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Coding the Ikeda map</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Exercise (5 minutes):</b> complete the code chunk below to define the Ikeda map\n",
    "         \\begin{align}\n",
    "    x_{{k+1}}=1 + u(x_{k}\\cos t_{k}+y_{k}\\sin t_{k}) & &\n",
    "    y_{{k+1}}=u(x_{k}\\sin t_{k}+y_{k}\\cos t_{k})\\\\\n",
    "    t_k = 0.4 - \\frac{6}{1 + x_k^2 + y_k^2}\n",
    "    \\end{align}\n",
    "        of a point defined as a 2-D array.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Ikeda(X_0, u):\n",
    "    \"\"\"The array X_0 will define the initial condition and the parameter u controls the chaos of the map\n",
    "    \n",
    "    This should return X_1 as the forward state.\"\"\"\n",
    "\n",
    "    t_1 =  # define the t1 here\n",
    "    \n",
    "    x_1 = # define the forward x state here\n",
    "    y_1 = # define the forward y state here\n",
    "                 \n",
    "    X_1 = np.array([x_1, y_1])\n",
    "    \n",
    "    return X_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> One example solution</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Ikeda(X_0, u):\n",
    "    \"\"\"The array X_0 will define the initial condition and the parameter u controls the chaos of the map\n",
    "    \n",
    "    This should return X_1 as the forward state.\"\"\"\n",
    "    \n",
    "    t_1 = 0.4 - 6 / (1 + X_0.dot(X_0) )\n",
    "    \n",
    "    x_1 = 1 + u * (X_0[0] * np.cos(t_1) + X_0[1] * np.cos(t_1))\n",
    "    y_1 = u * (X_0[0] * np.sin(t_1) + X_0[1] * np.cos(t_1))\n",
    "                 \n",
    "    X_1 = np.array([x_1, y_1])\n",
    "    \n",
    "    return X_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The computational example</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">In this example, to make plots, we will import the basic plotting library for python \"pyplot\" as a new object.</li>\n",
    "    <li class=\"fragment\">This object will, by convention, be called \"plt\".</li>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>    \n",
    "    <li class=\"fragment\">One slider in the example change the parameter value $u$ alter the dynamics.</li>\n",
    "    <li class=\"fragment\">The other slider changes the number of iterations of the initial condition that are plotted in the figure.</li>\n",
    "    <li class=\"fragment\"><b>Q:</b> what do you notice about the differences in the asymptotic trajectory as the value of $u$ is changed?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     7
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "\n",
    "def animate_ikeda(u=0.9, k=2):\n",
    "    \n",
    "    X_traj = np.zeros([k, 2])\n",
    "    X_traj[0,:] = [0,0]\n",
    "    for i in range(k-1):\n",
    "        tmp = Ikeda(X_traj[i, :], u)\n",
    "        X_traj[i+1, :] = tmp\n",
    "\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.add_axes([.1, .1, .8, .8])\n",
    "    ax.scatter(X_traj[:,0], X_traj[:, 1])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "w = interactive(animate_ikeda,u=(0,.95,0.01), k=(2, 2002, 50))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>3D-VAR in the Ikeda model</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">  We will now consider the problem of finding the (naive) maximum a posteriori state of the Ikeda model from</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\"> a background state, generated by a model forecast; and</li>\n",
    "        <li class=\"fragment\"> an observation of the \"true\" state with noise.</li>\n",
    "    </ol>\n",
    "    <li class=\"fragment\"> The 3D-VAR cost function once again takes the form of the weighted-least-squares difference between the two sources of information:\n",
    "    \\begin{align}\n",
    "    J(\\mathbf{x}) &= \\frac{1}{2}\\left[\\left(\\mathbf{x} - \\mathbf{x}_b\\right)^\\mathrm{T} \\mathbf{B}^{-1}\\left(\\mathbf{x} - \\mathbf{x}_b\\right) + \\left(\\mathbf{H}\\mathbf{x} - \\mathbf{y}_k\\right)^\\mathrm{T} \\mathbf{R}^{-1} \\left(\\mathbf{H}\\mathbf{x} - \\mathbf{y}_k\\right)\\right]  \n",
    "        \\end{align}\n",
    "    </li>\n",
    "</ul>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Twin experiments</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> This type of experiment is known as a \"twin-experiment\", in which we will generate both the \"model-twin\" and the \"truth-twin\", to evaluate the strengths and the limitations of the DA method.</li>\n",
    "    <li class=\"fragment\"> The \"truth-twin\" is the sequence of model states that generate the \"observered\" pseudo-data;</li>\n",
    "        <ul>\n",
    "            <li class=\"fragment\"> this pseudo-data is given to the DA method (possibly sequentially or all at once) to estimate the true sequence of states.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\"> The \"model-twin\" is the sequence of model states that are generated by the DA cycle;</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">the model twin is produced by using the numerical model to make a forecast and by analyzing the observations to produce analyses.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Coding 3D-VAR</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> <b>Exercise (3 minutes):</b> complete the code chunk below to define the 3D-VAR cost function:\n",
    "        \\begin{align}\n",
    "    J(\\mathbf{x}) &= \\frac{1}{2}\\left[\\left(\\mathbf{x} - \\mathbf{x}_b\\right)^\\mathrm{T} \\mathbf{B}^{-1}\\left(\\mathbf{x} - \\mathbf{x}_b\\right) + \\left(\\mathbf{H}\\mathbf{x} - \\mathbf{y}_k\\right)^\\mathrm{T} \\mathbf{R}^{-1} \\left(\\mathbf{H}\\mathbf{x} - \\mathbf{y}_k\\right)\\right]  \n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">Note that the inverse of a matrix can be called as a method as follows</li>        \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "A_inverse = np.linalg.inv(A)\n",
    "A_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def D3_var(X, args):\n",
    "    \"\"\"This function defines is the 3D-VAR cost function\n",
    "    \n",
    "    For simplicity, we will assume that the observation operator H is the identity operator\"\"\"\n",
    "    \n",
    "    # we unpack the extra arguments\n",
    "    [x_b, B, y_obs, R] = args\n",
    "    \n",
    "    b_diff = # define the weighted difference of the state from the background\n",
    "\n",
    "    W_innovation = # define the weighted difference of the state from the observation\n",
    "    \n",
    "    return b_diff + W_innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Coding 3D-VAR</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> <b>Example solution:</b> </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def D3_var(X, args):\n",
    "    \"\"\"This function defines is the 3D-VAR cost function\n",
    "    \n",
    "    For simplicity, we will assume that the observation operator H is the identity operator\"\"\"\n",
    "    \n",
    "    # we unpack the extra arguments\n",
    "    [x_b, B, y_obs, R] = args\n",
    "    \n",
    "    # define the weighted difference of the state from the background\n",
    "    b_diff = (X - x_b).transpose() @ np.linalg.inv(B) @ (X - x_b)\n",
    "\n",
    "    # define the weighted difference of the state from the observation\n",
    "    W_innovation = (y_obs - X).transpose() @ np.linalg.inv(R) @ (y_obs - X)\n",
    "    \n",
    "    return b_diff + W_innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Coding 3D-VAR continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">In order to implement the 3D-VAR method, we need to perform a numerical optimization/ root finding.</li>\n",
    "    <li class=\"fragment\">In this case, we need to call a method in order to find the zero of the cost function $J(x)$.</li>\n",
    "    <li class=\"fragment\">Scipy has a built in module called \"optimize\", from which we will import a root finding scheme.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "    <li class=\"fragment\">Additionally, for graphical tools in visualizing the covariances, we will import \"Ellipse\" from the module \"patches\" in matplotlib.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Analyzing 3D-VAR</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Exercise (2 minutes):</b>In the following cell, use the sliders to analyze the performance of the 3D-VAR estimator.  Specifically consider:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">What is the effect on the analysis solution by changing the variance of the background covariance $\\mathbf{B}\\triangleq B_{var} * \\mathbf{I}_2$?</li>\n",
    "                <li class=\"fragment\">What is the effect on the analysis solution by changing the variance of the observation error covariance $\\mathbf{B}\\triangleq R_{var} * \\mathbf{I}_2$?</li>\n",
    "                <li class=\"fragment\">What is the effect on the analysis solution by changing the number of analyses $N$?</li>\n",
    "    </ol>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def animate_D3(B_var = 0.1, R_var = 0.1, k=2):\n",
    "\n",
    "    # define the static background and observational error covariances\n",
    "    B = B_var * np.eye(2)\n",
    "    R = R_var * np.eye(2)\n",
    "\n",
    "    # set a random seed for the reproducibility\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # we define the mean for the background\n",
    "    x_b = np.array([0,0])\n",
    "    \n",
    "    # and the initial condition of the real state as a random draw from the prior\n",
    "    x_t = np.random.multivariate_normal([0,0], np.eye(2) * B_var)\n",
    "\n",
    "    # define the Ikeda map parameter\n",
    "    u = 0.75\n",
    "    for i in range(k-1):\n",
    "        \n",
    "        # we forward propagate the true state\n",
    "        x_t = Ikeda(x_t, u)\n",
    "        \n",
    "        # and generate a noisy observation\n",
    "        y_obs = x_t + np.random.multivariate_normal([0,0], R)\n",
    "        \n",
    "        # forward propagate the last analysis\n",
    "        x_b_f = Ikeda(x_b, u)\n",
    "        \n",
    "        # define the arguments necessary for the 3D-VAR\n",
    "        ARGS = [x_b_f, B, y_obs, R]\n",
    "\n",
    "        analys = minimize(D3_var, x_b_f, args=ARGS)\n",
    "        x_b = analys.x\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.add_axes([.1, .1, .8, .8])\n",
    "    \n",
    "    l1 = ax.scatter(x_b_f[0], x_b_f[1], c='k', s=40)\n",
    "    ax.add_patch(Ellipse(x_b_f, B_var, B_var, ec='k', fc='none'))\n",
    "    \n",
    "    \n",
    "    l2 = ax.scatter(y_obs[0], y_obs[1], c='r', s=40)\n",
    "    ax.add_patch(Ellipse(y_obs, R_var, R_var, ec='r', fc='none'))\n",
    "    \n",
    "    l3 = ax.scatter(x_b[0], x_b[1], c='b', s=40)\n",
    "    ax.set_xlim([-1, 4])\n",
    "    ax.set_ylim([-3,1])\n",
    "    \n",
    "    labels = ['Forecast', 'Observation', 'Analysis']\n",
    "    plt.legend([l1,l2,l3],labels, loc='upper right', fontsize=26)\n",
    "    plt.show()\n",
    "    \n",
    "w = interactive(animate_D3,B_var=(0.01,1.0,0.01), R_var=(0.01,1.0,0.01), k=(2, 50, 1))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Analyzing 3D-VAR continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">We can see that the analysis is closer to the background or the observation depending on the weights we give them, based on the relative uncertainty described in the naive Bayesian update.</li>\n",
    "    <li class=\"fragment\">However, there is no real performance gain from one analysis to the next, as the form of the naive cost function doesn't accumulate new information;</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">indeed, by re-using the same background prior at every step, we forget all information we gained in the posterior except for the last analysis state.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\"> This is the greatest weakness of the 3D-VAR approach, that it doesn't take into account the \"flow-dependence\" of the last posterior when generating a new prior.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>An extension --- 4D-VAR</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">One successful means of adding information to the cost function is to require that the maximum a posteriori solution doesn't deviate from a <em>sequence of observations</em>, under the constraint of the model evolution.</li>\n",
    "    <li class=\"fragment\">This approach is the basis of 4D-VAR, where the 4th dimension stands for time.</li>\n",
    "    <li class=\"fragment\">Let us suppose that we have a sequence of observations at times $t_1$ to $t_N$.</li>\n",
    "    <li class=\"fragment\">We will define this sequence as $\\{\\mathbf{y}_k\\}_{k=1}^N$, and we will assume we have an initial background state at time $t_0$ defined as $\\mathbf{x}_b$.</li>\n",
    "    <li class=\"fragment\">The 4D-VAR cost function is defined,\n",
    "        \\begin{align}\n",
    "       J\\left(\\mathbf{x}_0\\right)& = \\frac{1}{2}\\left(\\mathbf{x}_0 - \\mathbf{x}_b\\right)^\\mathrm{T} \\mathbf{B}^{-1}\\left(\\mathbf{x}_0 - \\mathbf{x}_b\\right) +  \\\\\n",
    "        &\\frac{1}{2}\\sum_{k=1}^N  \\left(\\mathbf{H}\\mathbf{x}_k - \\mathbf{y}_k\\right)^\\mathrm{T} \\mathbf{R}^{-1} \\left(\\mathbf{H}\\mathbf{x}_k - \\mathbf{y}_k\\right)\n",
    "        \\end{align}\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>4D-VAR continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">This is the approach to DA which has been adopted as the primary method at the European Centre for Medium-Range Weather Forecasts (ECMWF).</li>\n",
    "    <li class=\"fragment\"> This approach has been extremely successful, but has limitations due to the delicate nature of taking the derivative $\\partial_{\\mathbf{x}_0}$ of the 4D-VAR cost function\n",
    "        \\begin{align}\n",
    "       J\\left(\\mathbf{x}_0\\right)& = \\frac{1}{2}\\left(\\mathbf{x}_0 - \\mathbf{x}_b\\right)^\\mathrm{T} \\mathbf{B}^{-1}\\left(\\mathbf{x}_0 - \\mathbf{x}_b\\right) +  \\\\\n",
    "        &\\frac{1}{2}\\sum_{k=1}^N  \\left(\\mathbf{H}\\mathbf{x}_k - \\mathbf{y}_k\\right)^\\mathrm{T} \\mathbf{R}^{-1} \\left(\\mathbf{H}\\mathbf{x}_k - \\mathbf{y}_k\\right)\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">Particularly, taking the derivative with respect to the initial condition means that we must take the derivative of the equations of motion of the physics-based model with respect to the evolution of the initial condition.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>4D-VAR continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">It can be shown that if: </li>\n",
    "        <ol>\n",
    "            <li class=\"fragment\"> the matrix $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ is symmetric; </li>\n",
    "                <li class=\"fragment\">if the functional $J$ is defined as $J(\\mathbf{x})\\triangleq \\frac{1}{2} \\mathbf{y}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$; and</li>\n",
    "            <li class=\"fragment\">$\\mathbf{y}= \\mathbf{y}(\\mathbf{x})$;</li>\n",
    "    </ol>\n",
    "        <li class=\"fragment\">then, we have the partial derivative\n",
    "            \\begin{align}\n",
    "            \\frac{\\partial J}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}^\\mathrm{T} \\mathbf{A}\\mathbf{y}.\n",
    "            \\end{align}</li>\n",
    "</ul>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  \n",
    "<h3>4D-VAR continued</h3>\n",
    "<ul>\n",
    "        <li class=\"fragment\"> Using the previous rule as motivation, the full gradient is approximated by,\n",
    "        \\begin{align}\n",
    "        \\nabla_{\\mathbf{x}} J \\approx - \\mathbf{B}^{-1} \\left(\\mathbf{x}_b - \\mathbf{x} \\right) + \\sum_{k=1}^N \\left(\\mathbf{M}_k \\mathbf{M}_{k-1} \\cdots \\mathbf{M}_1 \\right)^\\mathrm{T} \\left( \\frac{\\partial \\mathbf{H}_k}{\\partial\\mathbf{x}_k}\\right)\\mathbf{R}^{-1}\\left(\\mathbf{y}_k - \\mathbf{H}_k\\mathbf{x}_k\\right)\n",
    "            \\end{align}</li>\n",
    "<li class=\"fragment\"> To solve for the minimum of the cost function by this approximation, the increments between the forward state and the associated observation at each time are iteratively minimized by the \"adjoint method\".</li> \n",
    "    <li class=\"fragment\">The adjoint model takes the future sensitivities back-in-time to earlier times, contra-variantly to the tangent-linear model.</li>\n",
    "     <li class=\"fragment\">For a useful discussion on the adjoint method, and its use in 4D-VAR, see e.g., the following  <a href=\"http://www.met.reading.ac.uk/~ross/Documents/Var4d.html\" target=\"blank\">tutorial on 4D-VAR</a>.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>4D-VAR continued</h3>\n",
    "<div style=\"float:left; width:60%\">\n",
    "<ul>\n",
    "    <li class=\"fragment\">The difficulty of implementing the 4D-VAR formulation has limited its adoption in operational DA, and makes this approach beyond the scope of this tutorial.</li>\n",
    "    <li class=\"fragment\"> We mention this approach here because of the theoretical and historical importance of this approach;</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\"> also, these techniques are increasingly being merged with statistical techniques into \"hybrid\" schemes in state-of-the-art methods.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\"> Understanding the statistical approach will be the subject of the remainder of these tutorials.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"float:left; width:40%\">\n",
    "    <img src=\"./4D-Var.jpg\"/>\n",
    "    <b>Image courtesy of <a href=\"https://www.ecmwf.int/en/learning/seminars/symposium-20-years-4dvar\">ECMWF</a> \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
