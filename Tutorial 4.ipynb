{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Mathematical Theory of Data Assimilation with Applications:<br>\n",
    "\n",
    "<p class=\"fragment\">Tutorial part 4 of 4 --- Bayesian DA through sampling<p></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Recall from last time</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">While we found a recursive parametric form for the next prior distribution in terms of the last posterior distibution, this method had limitations.  Particularly:</li>\n",
    "    <ol>\n",
    "         <li class=\"fragment\">The recursion only holds for models with linear dynamic and observational models and Gaussian error distributions.</li>\n",
    "        <li class=\"fragment\">In nonlinear systems, this suggested</li>\n",
    "        <ol>\n",
    "           <li class=\"fragment\">  evolving the mean state with the fully nonlinear equations; and </li>\n",
    "           <li class=\"fragment\"> hoping that the evolution of the last posterior is well represented by the evolution under the Jacobian equations along this trajectory.</li>\n",
    "        </ol>\n",
    "    </ol>\n",
    "        </ul>\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   \n",
    " <h3>Recall from last time</h3>\n",
    "\n",
    "<ul>   \n",
    "        <li class=\"fragment\">The linearization of the model in the extended Kalman filter is, however, expensive and is infeasible for operational models.</li>\n",
    "            <li class=\"fragment\">Moreover, the parametric form for this analysis is highly rigid, or <em>biased</em> in its learning;</li>\n",
    "        <ul>\n",
    "         <li class=\"fragment\">it assumes linear-Gaussian models and when the assumptions aren't well satisfied, the parametric form for the recursion of the posterior can diverge catastrophically.</li>\n",
    "            </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Sampling</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">One way to rectify the above issues is to estimate the posterior either non-parametrically (particle filters) or less-parametrically (ensemble Kalman filter).</li>\n",
    "    <li class=\"fragment\">When the goal is to obtain an accurate representation of the posterior distribution, we can consider the representation of the density as (singular) volumes with weights.</li>\n",
    "    <li class=\"fragment\">We will suppose for the moment that we have a target posterior distribution $p(\\mathbf{x}\\vert \\mathbf{y})$ that we can somehow draw independent samples from, even if we do not know its exact functional form.</li>\n",
    "    <li class=\"fragment\">Let $\\mathbf{x}^i\\in \\mathbb{R}^n$ denote the $i$-th sample drawn iid from this distribution.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Sampling continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"> If we draw $N$ independent samples from the distribution $p(\\mathbf{x}\\vert \\mathbf{y})$, an empirical representation of the distribution is given by\n",
    "    \\begin{align}\n",
    "        p_N(\\mathbf{x}\\vert \\mathbf{y}) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{\\mathbf{x}^i}\\left(\\mathrm{d}\\mathbf{x}\\right)\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">In the above, the denominator $\\frac{1}{N}$ represents that all point volumes have equal mass or weight, so that the total density integrates to one.</li>\n",
    "    <li class=\"fragment\">Then for any statistic $f$ of the posterior, we can recover an estimate of its expected value directly as\n",
    "        \\begin{align}\n",
    "        \\mathbb{E}_{p(\\mathbf{x}\\vert \\mathbf{y})} \\left[f \\right] \\triangleq\\int f p(\\mathbf{x}\\vert \\mathbf{y} ) \\mathrm{d}\\mathbf{x} \\approx \\int f p_N(\\mathbf{x}\\vert \\mathbf{y}) \\mathrm{d}\\mathbf{x} \n",
    "        &= \\frac{1}{N}\\sum_{i=1}^N f(\\mathbf{x})\\delta_{\\mathbf{x}^i}(\\mathrm{d}\\mathbf{x}).\n",
    "        \\end{align}\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Empirical estimates</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">The empirical estimate discussed before is also an unbiased estimator of the statistic $f$;</li>\n",
    "    <li class=\"fragment\">If the posterior variance of $f(\\mathbf{x})$ satisfies,\n",
    "            \\begin{align}\n",
    "            \\sigma^2_f = \\mathbb{E}_{p(\\mathbf{x}\\vert \\mathbf{y})}\\left[f^2(\\mathbf{x})\\right] - \\left(\\mathbb{E}_{p(\\mathbf{x}\\vert \\mathbf{y})}\\left[f\\right]\\right)^2 < \\infty;\n",
    "             \\end{align}\n",
    "             </li>\n",
    "    <li class=\"fragment\"> then the variance of the empirical estimate, \\begin{align}\n",
    "        var\\left(\\mathbb{E}_{p_N(\\mathbf{x}\\vert \\mathbf{y})}\\left[f\\right]\\right) = \\frac{\\sigma_f^2}{N},\\end{align}\n",
    "        (where the variance is understood as taken over the possible sample outcomes).</li>\n",
    "     <li class=\"fragment\">If $\\sigma_f^2$ is also finite, then by the central limit theorem we know,\n",
    "         \\begin{align}\n",
    "         \\lim_{N\\rightarrow +\\infty}\\sqrt{N}\\left\\{ \\mathbb{E}_{p(\\mathbf{x}\\vert \\mathbf{y})}\\left[f\\right] - \\mathbb{E}_{p_N(\\mathbf{x}\\vert \\mathbf{y})}\\left[f\\right]\\right\\} =N(0, \\sigma_f^2),\n",
    "         \\end{align}\n",
    "         i.e., the empirical distribution converges to the true distribution in the weak sense as $N$ gets sufficiently large.</li>\n",
    "    \n",
    " </ul>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Importance sampling</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"> In practice, we often cannot sample the posterior directly but we may need to sample some other distribution that shares its support.</li>\n",
    "    <li class=\"fragment\">This idea of sampling another distribution with shared support is known as <em>importance sampling</em>.\n",
    "        <li class=\"fragment\">We will suppose that we have access, perhaps not to $p(\\mathbf{x}\\vert \\mathbf{y})$ but instead $\\pi(\\mathbf{x}\\vert \\mathbf{y})$ such that $p \\ll \\pi$,</li>\n",
    "    <ul>\n",
    "            <li class=\"fragment\"> i.e., $p(A\\vert \\mathbf{y})>0 \\Rightarrow \\pi(A\\vert \\mathbf{y})>0$.\n",
    "    </ul>\n",
    "    <li class=\"fragment\">This above assumption allows us to take the Radon-Nikodym derivative of the true posterior $p(\\mathbf{x}\\vert \\mathbf{y})$ with respect to the <em>proposal distribution</em> $\\pi(\\mathbf{x}\\vert \\mathbf{y})$.</li>\n",
    "    <li class=\"fragment\">The key innovation to the last formulation is that this allows us to evaluate a statistic of the posterior by point volumes but with non-equal <em>importance weights</em>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Importance sampling continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"> Let us define the importance weight function $w(\\mathbf{x}) \\triangleq \\frac{p(\\mathbf{x}\\vert \\mathbf{y})}{\\pi(\\mathbf{x}\\vert \\mathbf{y})}$.</li>\n",
    "    <li class=\"fragment\">The we can re-write the expected value of some statistic $f$ of the posterior as,\n",
    "        \\begin{align}\n",
    "        \\mathbb{E}_{p(\\mathbf{x}\\vert \\mathbf{y})}[f]&=\\frac{\\int f(\\mathbf{x})w(\\mathbf{x})\\pi(\\mathbf{x}\\vert \\mathbf{y})\\mathrm{d}\\mathbf{x}}{\\int w(\\mathbf{x})\\pi(\\mathbf{x}\\vert \\mathbf{y})\\mathrm{d}\\mathbf{x}} \n",
    "        \\end{align}\n",
    "        </li>\n",
    "    <li class=\"fragment\">The benefit for sampling techniques is therefore to take iid $\\mathbf{x}^i \\sim \\pi(\\mathbf{x}\\vert \\mathbf{y})$ and to write the empirically derived expected value of $f$ as, \n",
    "        \\begin{align}\n",
    "        \\mathbb{E}_{p_N(\\mathbf{x}\\vert \\mathbf{y})}[f] = \\frac{ \\frac{1}{N} \\sum_{i=1}^N f(\\mathbf{x}^i)w(\\mathbf{x}^i)}{\\frac{1}{N} \\sum_{i=1}^N w(\\mathbf{x}^i)} = \\sum_{i=1}^N f(\\mathbf{x}^i) \\tilde{w}^i.\n",
    "        \\end{align}\n",
    "        </li>\n",
    "    <li class=\"fragment\">Here, the $\\tilde{w}^i\\triangleq \\frac{w(\\mathbf{x}^i)}{\\sum_{i=1}^N w(\\mathbf{x}^i)}$ are defined as the <em>normalized importance weights</em>;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Importance sampling continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Q:</b> if we take $f(\\mathbf{x}) = 1$, what is the expected value based on the empirical measure with sampling weights? Why is this important?</li>\n",
    "    <li class=\"fragment\"><b>A:</b> the expected value is one --- this means that the weighted point volumes gives a probability measure.</li>\n",
    "    <li class=\"fragment\">Particularly, in this way, we will write our empirical estimate of the posterior as,\n",
    "        \\begin{align}\n",
    "        p_N(\\mathbf{x}\\vert \\mathbf{y}) \\triangleq \\sum_{i=1}^N \\tilde{w}^i\\delta_{\\mathbf{x}^i}(\\mathrm{d}\\mathbf{x}).\n",
    "        \\end{align}</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Importance sampling continued</h3>\n",
    "\n",
    "<ul>\n",
    "    </li>\n",
    "    <li class=\"fragment\">Using this formulation, we now have an extremely flexible view of the posterior as combination of positions (samples) and weights (probabilities).</li>\n",
    "    <li class=\"fragment\">In the DA problem, we once again have a natural choice of how to find the next prior from the last posterior; </li>\n",
    "    <ul>\n",
    "    <li class=\"fragment\"> this is done by evolving the points and (possibly) finding new weights or resampling positions at the moment of conditioning on new observed information.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Sequential importance sampling</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">Key to creating an operational algorithm is to extend importance sampling into a sequential algorithm.</li>\n",
    "    <li class=\"fragment\">Particularly, when we extend the posterior to a squence of model forecast states conditioned on a sequence of observed states, \n",
    "        \\begin{align}\n",
    "        \\mathbf{x}_{0:k}&\\triangleq \\left\\{\\mathbf{x}_i : i = 0, \\cdots ,k\\right\\},\\\\\n",
    "        \\mathbf{y}_{0:k}&\\triangleq \\left\\{\\mathbf{y}_i : i = 0, \\cdots ,k\\right\\},\n",
    "        \\end{align}\n",
    "    </li>   \n",
    "    <li class=\"fragment\">we will wish to find a recursive formulation of for the posterior, like we did in the Kalman filter.\n",
    "    </li>\n",
    "    <li class=\"fragment\">In order to formulate this, we will marginalize over the previous states, i.e.,\n",
    "        \\begin{align}\n",
    "        p\\left(\\mathbf{x}_{0:k}\\vert \\mathbf{y}_{0:k}\\right) = p\\left(\\mathbf{x}_{0:k-1}\\vert \\mathbf{y}_{0:k}\\right) p\\left(\\mathbf{x}_k \\vert \\mathbf{x}_{0:k-1}, \\mathbf{y}_{0:k}\\right).\n",
    "        \\end{align}</li>\n",
    "    <li class=\"fragment\">Iterating on this formula recursively, we find that,\n",
    "        \\begin{align}\n",
    "        p(\\mathbf{x}_{0:k}\\vert \\mathbf{y}_{0:k}) = p(\\mathbf{x}_0) \\prod_{m=1}^k p(\\mathbf{x}_m \\vert \\mathbf{x}_{0:m-1}, \\mathbf{y}_{0:k})\n",
    "        \\end{align}\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Sequential importance sampling continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">Using the recursion and using the Markovian assumption ($p(\\mathbf{x}_{m}\\vert \\mathbf{x}_{0:m-1}) = p(\\mathbf{x}_m \\vert \\mathbf{x}_{m-1})$), we find the recursive form for the weights via Bayes' Law as,\n",
    "    \\begin{align}\n",
    "        \\tilde{w}^i_k \\propto \\tilde{w}^i_{k-1} \\frac{p\\left(\\mathbf{y}_k \\vert \\mathbf{x}^i_k\\right) p\\left(\\mathbf{x}^i_k \\vert \\mathbf{x}^i_{k-1}\\right)}{\\pi\\left(\\mathbf{x}_k^i \\vert \\mathbf{x}^i_{0:k-1}, \\mathbf{y}_{0:k} \\right)}.\n",
    "    \\end{align}\n",
    "    </li>\n",
    " <li class=\"fragment\">Quintessentially, we will assume that $\\pi$ is the prior $p(\\mathbf{x}_{0:k})$; in this case, we have the weights given recursively by $\\tilde{w}^i_k \\propto \\tilde{w}^i_{k-1} p(\\mathbf{y}_k \\vert \\mathbf{x}_k^i)$.</li>\n",
    "    <li class=\"fragment\">The proportionality statement says that:\n",
    "        <ul>\n",
    "        <li class=\"fragment\"> after conditioning each weight on the new observation, we need just re-normalize the weights to sum to one in order to recover the empirical posterior.</li>\n",
    "        </ul>\n",
    " </ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Sequential importance sampling continued</h3>\n",
    "\n",
    "<ul>    \n",
    "        <li class=\"fragment\">Sequential importance sampling for estimating the posterior is extremely flexible and makes few assumptions on the form of the problem whatsoever;</li>\n",
    "    <li class=\"fragment\">however, the primary issue that arises is that the importance weights become extremely skewed extremely quickly, leading to all the probability mass landing on a single point after only a few iterations.</li>\n",
    "    <li class=\"fragment\">Methods for handling the degeneracy of the weights is explicitly the motivation for the <em>bootstrap particle filter</em>, and implicitly one of the motivations for the <em>ensemble Kalman filter</em>.</li>\n",
    "        <li class=\"fragment\"> The method of the bootstrap filter essentially proposes to eliminate the degeneracy of the weights by eliminating samples with weights close to zero and resampling.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The bootstrap filter</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">At the point of the Bayesian update and re-weighting:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">eliminate all samples with weights $\\tilde{w}^i < W$ where $W\\ll 1$ will be some threshold for the weights;</li> \n",
    "        <li class=\"fragment\"> in turn, we make replicates of the higher weighted samples and reset the importance weights all equal to $\\frac{1}{N}$;</li>\n",
    "    <li class=\"fragment\">then the new empirical measure is then given by,\n",
    "        \\begin{align}\n",
    "        p_N(\\mathrm{d}\\mathbf{x}_{0:k}\\vert \\mathbf{y}_{0:k}) = \\frac{1}{N} \\sum_{i=1}^N N^i \\delta_{\\mathbf{x}^i}(\\mathrm{d}\\mathbf{x}),\n",
    "        \\end{align}\n",
    "        where $N^i$ is the number of replicates ($N^i\\in[0, N]$) of sample $\\mathbf{x}^i$ such that $\\sum_{i=1}^N N^i =N$</li>\n",
    "    </ol>\n",
    "    <li class=\"fragment\">How the number of replicates $N^i$ is chosen is the basis of several different approaches to particle filters.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The bootstrap filter continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"> The classical bound for how well the empirical measure approximates the true posterior can be stated as follows:</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">Suppose $f$ is a bounded function (statistic) with $\\parallel f\\parallel\\triangleq \\sup_{\\mathbf{x}_{0:k} } \\rvert f(\\mathbf{x}_{0:k})\\rvert$,</li> \n",
    "        <li class=\"fragment\"> then there exists some $C>0$ and independent of $k$ such that\n",
    "        \\begin{align}\n",
    "        \\mathbb{E} \\left[ \\left\\{ \\int f(\\mathbf{x}_{0:k}) p(\\mathrm{d}\\mathbf{x}_{0:k}\\vert \\mathbf{y}_{0:k}) - \\int f(\\mathbf{x}_{0:k}) p_N(\\mathrm{d}\\mathbf{x}_{0:k}\\vert \\mathbf{y}_{0:k}) \\right\\}^2 \\right] \\leq \\frac{C \\parallel f \\parallel^2}{N}.\n",
    "        \\end{align}\n",
    "    </li>\n",
    "        <li class=\"fragment\"><b>Note:</b> the constant $C$ grows exponentially in the dimension $d$, $\\mathbf{y}_k \\in \\mathbb{R}^d$.</li>\n",
    "        <li class=\"fragment\"><b>Exercise (3 minutes):</b> use Jensen's inequality to derive the rate at which the empirical measure converges to the true measure (in the number of samples N) in the expectation of the absolute difference.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The bootstrap filter continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Solution:</b> Jensen's inequality states that for a convex function $\\phi$ on the real line, if $(\\Omega, \\mathcal{A}, \\mu)$ is a probability space and $g$ is a real-valued integrable function,\n",
    "        \\begin{align}\n",
    "        \\phi\\left(\\int_\\Omega g \\mathrm{d}\\mu\\right) \\leq \\int_\\Omega \\phi\\circ g \\mathrm{d}\\mu.\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">We consider $\\phi$ defined $\\phi(x) = x^2$;</li> \n",
    "        <li class=\"fragment\"> let us define $g$ as,\n",
    "        \\begin{align}\n",
    "            g(\\omega)\\triangleq \\left\\rvert \\int f(\\mathbf{x}_{0:k}) p(\\mathrm{d}\\mathbf{x}_{0:k}\\vert \\mathbf{y}_{0:k}) - \\int f(\\mathbf{x}_{0:k}) p_N(\\mathrm{d}\\mathbf{x}_{0:k}\\vert \\mathbf{y}_{0:k}) \\right\\rvert,\n",
    "        \\end{align}\n",
    "            where $\\omega$ is some outcome.</li>    \n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The bootstrap filter continued</h3>\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li class=\"fragment\">Then,\n",
    "        \\begin{align}\n",
    "        &\\phi\\left(\\mathbb{E} \\left[ g \\right]\\right) \\leq \\mathbb{E}[\\phi \\circ g ] \\leq \\frac{C \\parallel f \\parallel^2}{N}\\\\\n",
    "        \\Rightarrow & \\mathbb{E}\\left[g\\right] \\leq \\frac{\\sqrt{C} \\parallel f \\parallel}{\\sqrt{N}}\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The bootstrap filter continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"> The constant $C$ is problematic for most operational DA in the tradeoff it represents.  Particularly as:</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">the observational dimension for operational DA is up the order $\\mathcal{O}\\left(10^8\\right)$, we cannot feasibly control this constant with a denominator that grows at $\\sqrt{N}$ in the number of samples;</li>\n",
    "        <li class=\"fragment\">we can reduce the number of observations to better approximate the true posterior, but the true posterior we will represent empirically will be one that is <em>extremely deprived of information</em>.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\">Indeed, the state dimension in operational models is up to the order of $\\mathcal{O}\\left(10^9\\right)$, such that the inference problem is woefully underconstrained already;</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">if we reduce the ammount of observational information, we will get a good empirical estimate of an oblivious posterior.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Bias versus variance of estimators</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"> We will only mention that advanced learning techniques built upon the ideas from particle filters are an active area of research.</li>\n",
    "    <li class=\"fragment\">Typically, these techniques must offer some compensation for the extremely high variance of particle filters by introducing a form of bias into their estimates.</li>\n",
    "    <li class=\"fragment\">We will discuss the bias-variance tradeoff loosely in the following.</li>\n",
    "    <li class=\"fragment\">Generically, we suppose there is some unkown relationship $f$ that describes,\n",
    "        \\begin{align}\n",
    "        \\mathbf{y} = f(\\mathbf{x}) + \\epsilon\n",
    "        \\end{align}\n",
    "        where $\\mathbb{E}[\\epsilon] = 0$ and $var(\\epsilon) = \\sigma^2 <\\infty$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Bias versus variance of estimators continued</h3>\n",
    "\n",
    "<ul> \n",
    "    <li class=\"fragment\">Treated as supervised learning of the relationship, if we choose some approximate form $\\hat{f}$ for the true relationship $f$, </li>\n",
    "    <li class=\"fragment\">it can be demonstrated that the generalization error (outside of the training data) decomposes as\n",
    "        \\begin{align}\n",
    "        \\mathbb{E}\\left[\\left(y -\\hat{f}(x)\\right)^2\\right] &= \\left(\\mathbb{E}[\\hat{f}(x) - f(x)\\right)^2 + \\left(\\mathbb{E}[\\hat{f}(x)^2] - \\mathbb{E}[\\hat{f}(x)]^2\\right) + \\sigma^2\\\\\n",
    "        &= \\left\\{\\mathrm{Bias}\\left[\\hat{f}(x)\\right]\\right\\}^2 + var\\left[\\hat{f}(x)\\right] + \\sigma^2\n",
    "        \\end{align}</li>\n",
    "        <li class=\"fragment\">where (heuristically):</li> \n",
    "        <ul>\n",
    "            <li class=\"fragment\"> the bias represents the rigidness of the assumptions (e.g., linearity/Gaussianity) of the learning scheme; and \n",
    "            <li class=\"fragment\">the variance is the flexibility of the method and how far it will move from its mean when introduced to new data.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Bias versus variance of estimators continued</h3>\n",
    "\n",
    "<ul>    \n",
    "    <li class=\"fragment\">The particle filter has extremely high variance in its learning due to the flexibility of its assumptions;</li>\n",
    "    <li class=\"fragment\">the extended Kalman filter on the other hand had extremely high bias by enforcing Gaussian-linear assumptions in the entire analysis.</li>\n",
    "    <li class=\"fragment\"> This motivates one of the more successful approaches to DA: the ensemble Kalman filter (EnKF).</li>\n",
    "    <li class=\"fragment\">The EnKF can be seen in one sense as introducing variance into the extended Kalman filter by allowing the past-posterior to evolve via sampling into the next posterior nonlinearly.</li>\n",
    "    <li class=\"fragment\"> However, at the point of the update, we reintroduce bias into the sampling analysis by enforcing a Gaussian assumption (or in some versions a Gaussian mixture assumption).</li>\n",
    "    <li class=\"fragment\"> This combination is often enough to reduce the overall error by introducing an appropriate ammount of bias and variance.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Ensemble Kalman filter</h3>\n",
    "\n",
    "<ul>    \n",
    "    <li class=\"fragment\">The set up of the ensemble Kalman filter will begin similarly to the particle filter.</li>\n",
    "    <li class=\"fragment\">We suppose that at time $k$ we have $N$ iid samples of a prior, $\\left\\{\\mathbf{x}^{fi}_k\\right\\}_{i=1}^N$ which will form the columns of a matrix,\n",
    "        \\begin{align}\n",
    "        \\mathbf{X}^{f}_k &\\triangleq \\begin{pmatrix}\\mathbf{x}_k^{f1} & \\cdots & \\mathbf{x}^{fN}_k\\end{pmatrix}.\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">We define the mean of the ensemble as,\n",
    "        \\begin{align}\n",
    "        \\overline{\\mathbf{x}}^f_k \\triangleq \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}^{fi}_k,\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">and from the above, we define the matrix of the anomalies as,\n",
    "        \\begin{align}\n",
    "        \\mathbf{A}^f_k \\triangleq \\begin{pmatrix} \\mathbf{x}^{f1}_k - \\overline{\\mathbf{x}}^f_k & \\cdots & \\mathbf{x}^{fN}_k - \\overline{\\mathbf{x}}^f_k \\end{pmatrix}.\n",
    "        \\end{align}\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Ensemble Kalman filter continued</h3>\n",
    "\n",
    "<ul>    \n",
    "    <li class=\"fragment\">The anomalies are precisely given by the deviations of the samples from the mean, such that the (unbiased) sample-based prior covariance is given,\n",
    "        \\begin{align}\n",
    "        \\mathbf{P}^f_k = \\frac{1}{N-1} \\mathbf{A}^f \\left(\\mathbf{A}^f\\right)^\\mathrm{T} .\n",
    "        \\end{align}\n",
    "        </li>\n",
    "    <li class=\"fragment\">Suppose that we are provided an observation $\\mathbf{y}_k \\sim N(\\mathbf{H}\\mathbf{x}^t_k ,\\mathbf{R})$.</li>\n",
    "    <li class=\"fragment\">Rather than (re)-weighting the samples, as in the particle filter, we will resample the \"posterior\" assuming that the prior is Gaussian and a Gaussian likelihood for the observations.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The (stochastic) Ensemble Kalman filter continued</h3>\n",
    "\n",
    "<ul>    \n",
    "        <li class=\"fragment\">We will apply the gain to each ensemble member, but for theoretical reasons, we will perturb the observation by a noise realization for each ensemble member.</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">When we use a direct implementation of the Kalman gain, this is necessary to produce the \"correct\" analysis covariance;</li> \n",
    "        <ul>\n",
    "        <li class=\"fragment\">however, advanced methods use a transform of the ensemble directly to avoid this step.</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "    <li class=\"fragment\">We suppose we draw $N$ observation perturbations $\\boldsymbol{\\psi}^i \\sim N(0, \\mathbf{R})$</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">We will enforce that $\\frac{1}{N}\\sum_{i=1}^N \\boldsymbol{\\psi}^i=0$.</li>\n",
    "        <li class=\"fragment\">Furthermore, we define $\\hat{\\mathbf{R}}\\triangleq cov(\\boldsymbol{\\psi}^i)$ to be the ensemble based observation error covariance.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\">Then, we define the perturbed observations as $\\mathbf{y}^i_k \\triangleq \\mathbf{y}_k + \\boldsymbol{\\psi}^i$, such that $\\mathbf{y}^i_k \\sim N\\left(\\mathbf{y}_k, \\hat{\\mathbf{R}}\\right)$.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The (stochastic) Ensemble Kalman filter continued</h3>\n",
    "\n",
    "<ul> \n",
    "    <li class=\"fragment\">We can functionally define the Kalman gain as before, but with respect to the ensemble-based prior covariance $\\mathbf{P}^f_k$ and ensemble-based observation error covariance $\\hat{\\mathbf{R}}$.\n",
    "        \\begin{align}\n",
    "        \\mathbf{K}_k \\triangleq \\mathbf{P}^f_k \\mathbf{H}\\left(\\mathbf{H}\\mathbf{P}^f_k\\mathbf{H}^\\mathrm{T} + \\hat{\\mathbf{R}}\\right)^{-1}\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"><b>Note:</b> the Kalman gain isn't guaranteed to provide a recursion for the Bayesian posterior except in the linear-Gaussian case;</li> \n",
    "        <li class=\"fragment\">therefore in this case, we use the ensemble-based gain as a sub-optimal, biased estimator.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The (stochastic) Ensemble Kalman filter continued</h3>\n",
    "\n",
    "<ul> \n",
    "    <li class=\"fragment\">Finally, we approximate the Bayesian update using the ensemble-based gain as,\n",
    "    \\begin{align}\n",
    "        \\mathbf{x}^{ai}_k = \\mathbf{x}^{fi}_k + \\mathbf{K}\\left(\\mathbf{y}^i_k - \\mathbf{H}\\mathbf{x}^{fi}_k\\right)\n",
    "     \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">From the analysis samples, we can once again compute the first two moments to have an estimated \"best state\" and its uncertainty.</li>\n",
    "    <li class=\"fragment\">An arbitrary statistic $f$ of the posterior can be computed by,\n",
    "        \\begin{align}\n",
    "        \\mathbb{E}[f] = \\frac{1}{N} \\sum_{i=1}^N f(\\mathbf{x}^{ai}_k),\n",
    "        \\end{align}\n",
    "        such that we see that the samples are all given equal weight.</li>\n",
    "    <li class=\"fragment\">To produce the next prior, we forecast each sample in the fully nonlinear numerical model.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Ensemble Kalman filter continued</h3>\n",
    "\n",
    "<ul>    \n",
    "    <li class=\"fragment\">Even with the Gaussian assumption, the EnKF is an extremely successful learning algorithm for nonlinear systems.</li>\n",
    "    <li class=\"fragment\"> However, techniques such as inflation and localization can be seen as introducing more variance into the estimator, relaxing the bias in the EnKF.</li>\n",
    "    <li class=\"fragment\">In addition, we may introduce hyper-priors for values such as $\\mathbf{Q}$ and $\\mathbf{R}$ to similarly increase the variance of the estimator.</li>\n",
    "    <li class=\"fragment\"> We will explore a simple example with the EnKF in the following.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Stochastic EnKF in the Ikeda map</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Exercise (3 minutes):</b>We will now examine the performance of the EnKF, using once again in the Ikeda map in a twin experiment.</li>\n",
    "    <li class=\"fragment\">In the following code, we will plot the forecast and analysis <em>ensembles</em> to demonstrate how they track the observations.</li>\n",
    "    <li class=\"fragment\">The mean of each ensemble will be plotted as a diamond, while ensemble members will be plotted as opaque points.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Stochastic EnKF in the Ikeda map</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">We want to evaluate the following questions:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">How does the performance of the EnKF change with the number of samples?</li>\n",
    "        <li class=\"fragment\">How does the performance of the EnKF change over the number of assimilation steps?</li>\n",
    "        <li class=\"fragment\">How does the performance of the EnKF change with respect to the initial prior uncertainty $B_{var}$?</li>\n",
    "        <li class=\"fragment\">How does the performance of the EnKF change with respect to the observational error variance $R_{var}$?</li>\n",
    "    </ol>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     20
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def Ikeda(X_0, u):\n",
    "    \"\"\"The array X_0 will define the initial condition and the parameter u controls the chaos of the map\n",
    "    \n",
    "    This should return X_1 as the forward state.\"\"\"\n",
    "    \n",
    "    t_n = 0.4 - 6 / (1 + X_0.dot(X_0) )\n",
    "    \n",
    "    x_1 = 1 + u * (X_0[0] * np.cos(t_n) + X_0[1] * np.cos(t_n))\n",
    "    y_1 = u * (X_0[0] * np.sin(t_n) + X_0[1] * np.cos(t_n))\n",
    "                 \n",
    "    X_1 = np.array([x_1, y_1])\n",
    "    \n",
    "    return X_1\n",
    "\n",
    "def Ikeda_V(X_0, u):\n",
    "    \"\"\"The array X_0 will define the ensemble matrix of dimension 2 times N_ens\n",
    "    \n",
    "    This should return X_1 as the forward state.\"\"\"\n",
    "    \n",
    "    t_n = 0.4 - 6 / (1 + np.sum(X_0*X_0, axis=0) )\n",
    "    \n",
    "    x_1 = 1 + u * (X_0[0, :] * np.cos(t_n) + X_0[1, :] * np.cos(t_n))\n",
    "    y_1 = u * (X_0[0, :] * np.sin(t_n) + X_0[1, :] * np.cos(t_n))\n",
    "                 \n",
    "    X_1 = np.array([x_1, y_1])\n",
    "    \n",
    "    return X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def animate_enkf(B_var = 0.1, R_var = 0.1, N=2, ens_n=3):\n",
    "\n",
    "    # define the static background and observational error covariances\n",
    "    P_0 = B_var * np.eye(2)\n",
    "    R = R_var * np.eye(2)\n",
    "\n",
    "    # set a random seed for the reproducibility\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # we define the mean for the background\n",
    "    x_b = np.array([0,0])\n",
    "    \n",
    "    # and the initial condition of the real state as a random draw from the prior\n",
    "    x_t = np.random.multivariate_normal([0,0], P_0)\n",
    "\n",
    "    y_obs = np.zeros([2,N-1])\n",
    "    \n",
    "    # define the Ikeda map parameter\n",
    "    u = 0.9\n",
    "    for i in range(N-1):\n",
    "        # we forward propagate the true state\n",
    "        x_t = Ikeda(x_t, u)\n",
    "    \n",
    "        # and generate a noisy observation\n",
    "        y_obs[:, i] = x_t + np.random.multivariate_normal([0,0], R)\n",
    "    \n",
    "    \n",
    "    # we define the ensemble as a random draw of the prior\n",
    "    ens = np.random.multivariate_normal(x_b, P_0, size=ens_n).transpose()\n",
    "\n",
    "    \n",
    "    # define the Ikeda map parameter\n",
    "    for i in range(N-1):\n",
    "        \n",
    "        # forward propagate the last analysis\n",
    "        ens_f = Ikeda_V(ens, u)\n",
    "        \n",
    "        # we generate observation perturbations\n",
    "        obs_perts =  np.random.multivariate_normal([0,0], R, size=ens_n)\n",
    "        obs_perts = obs_perts - np.mean(obs_perts, axis=0)\n",
    "        \n",
    "        # we generate the ensemble based observation error covariance \n",
    "        obs_cov = obs_perts.transpose() @ obs_perts / (ens_n - 1)\n",
    "        \n",
    "        # we perturb the observations\n",
    "        perts_obs = np.squeeze(y_obs[:,i]) + obs_perts\n",
    "        \n",
    "        # we compute the ensemble mean and anomalies\n",
    "        X_mean_f = np.mean(ens_f, axis=1)\n",
    "        A_t = (ens_f.transpose() - X_mean_f) / np.sqrt(ens_n - 1)\n",
    "        \n",
    "        # and the ensemble covariances\n",
    "        P = A_t.transpose() @ A_t\n",
    "\n",
    "        # we compute the ensemble based gain and the analysis ensemble\n",
    "        K_gain = P @ np.linalg.inv( P + obs_cov)\n",
    "        ens = ens_f + K_gain @ (perts_obs.transpose() - ens_f)\n",
    "        X_mean_a = np.mean(ens, axis=1)\n",
    "\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.add_axes([.1, .1, .8, .8])\n",
    "    \n",
    "    l1 = ax.scatter(ens_f[0, :], ens_f[1, :], c='k', s=20, alpha=.5, marker=',')\n",
    "    ax.scatter(X_mean_f[0], X_mean_f[1], c='k', s=200, marker=\"D\")\n",
    "    \n",
    "    l3 = ax.scatter(ens[0, :], ens[1, :], c='b', s=20, alpha=.5, marker=',')\n",
    "    ax.scatter(X_mean_a[0], X_mean_a[1], c='b', s=200, marker=\"D\")\n",
    "    \n",
    "    l2 = ax.scatter(y_obs[0, -1], y_obs[1, -1], c='r', s=20)\n",
    "    ax.add_patch(Ellipse(y_obs[:,-1], R_var, R_var, ec='r', fc='none'))\n",
    "    \n",
    "    \n",
    "    ax.set_xlim([-2, 4])\n",
    "    ax.set_ylim([-4,2])\n",
    "    \n",
    "    labels = ['Forecast', 'Observation', 'Analysis']\n",
    "    plt.legend([l1,l2,l3],labels, loc='upper right', fontsize=26)\n",
    "    plt.show()\n",
    "    \n",
    "w = interactive(animate_enkf,B_var=(0.01,1.0,0.01), R_var=(0.01,1.0,0.01), N=(2, 50, 1), ens_n=(3,300, 3))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Summary of the EnKF</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">The EnKF makes a vast improvement over the earlier explored methods and demonstrates its robustness as a learning scheme <em>when there are sufficiently many samples.</em></li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">It should be noted that this requires vastly fewer samples than implementing an effective bootstrap particle filter, but at the cost of introducing bias in the analysis of the posterior.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\">However in operational settings, the reality is that ensemble-based forecasting is still highly expensive and most operational EnKF uses at most $\\mathcal{O}\\left(10^2\\right)$ samples in the learning.</li>\n",
    "    <li class=\"fragment\">While the EnKF is highly parallelizable, numerical weather prediciton models require massive computation power and this fundamentally limits the number of available samples.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Making the EnKF work in practice</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">The reality of implementing the EnKF in a numerical weather prediction setting is that the covariance estimates will also be highly biased and extremely rank deficient.</li>\n",
    "    <li class=\"fragment\">While there are reasons to belive that the true Bayesian posterior covariance is also rank deficient, in the end we especially rely on:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">inflation; and</li>\n",
    "        <li class=\"fragment\">localization;</li>\n",
    "    </ol>\n",
    "    <li class=\"fragment\">in order to relax the error estimates (introduce variance) and rectify the extreme rank deficiency.</li>\n",
    "    <li class=\"fragment\">Both of these techniques are highly active research areas for improving ensemble-based filtering, which are discussed in, e.g., the recent review article of Carrassi et al.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
