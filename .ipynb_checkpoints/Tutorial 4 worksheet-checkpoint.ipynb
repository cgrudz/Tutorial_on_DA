{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Mathematical Theory of Data Assimilation with Applications:<br>\n",
    "\n",
    "<p class=\"fragment\">Tutorial part 4 of 4 --- Bayesian DA through sampling<p></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Stochastic EnKF in the Ikeda map</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Exercise (3 minutes):</b>We will now examine the performance of the EnKF, using once again in the Ikeda map in a twin experiment.</li>\n",
    "    <li class=\"fragment\">In the following code, we will plot the forecast and analysis <em>ensembles</em> to demonstrate how they track the observations.</li>\n",
    "    <li class=\"fragment\">The mean of each ensemble will be plotted as a diamond, while ensemble members will be plotted as opaque points.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Stochastic EnKF in the Ikeda map</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">We want to evaluate the following questions:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">How does the performance of the EnKF change with the number of samples?</li>\n",
    "        <li class=\"fragment\">How does the performance of the EnKF change over the number of assimilation steps?</li>\n",
    "        <li class=\"fragment\">How does the performance of the EnKF change with respect to the initial prior uncertainty $B_{var}$?</li>\n",
    "        <li class=\"fragment\">How does the performance of the EnKF change with respect to the observational error variance $R_{var}$?</li>\n",
    "    </ol>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     20
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def Ikeda(X_0, u):\n",
    "    \"\"\"The array X_0 will define the initial condition and the parameter u controls the chaos of the map\n",
    "    \n",
    "    This should return X_1 as the forward state.\"\"\"\n",
    "    \n",
    "    t_n = 0.4 - 6 / (1 + X_0.dot(X_0) )\n",
    "    \n",
    "    x_1 = 1 + u * (X_0[0] * np.cos(t_n) + X_0[1] * np.cos(t_n))\n",
    "    y_1 = u * (X_0[0] * np.sin(t_n) + X_0[1] * np.cos(t_n))\n",
    "                 \n",
    "    X_1 = np.array([x_1, y_1])\n",
    "    \n",
    "    return X_1\n",
    "\n",
    "def Ikeda_V(X_0, u):\n",
    "    \"\"\"The array X_0 will define the ensemble matrix of dimension 2 times N_ens\n",
    "    \n",
    "    This should return X_1 as the forward state.\"\"\"\n",
    "    \n",
    "    t_n = 0.4 - 6 / (1 + np.sum(X_0*X_0, axis=0) )\n",
    "    \n",
    "    x_1 = 1 + u * (X_0[0, :] * np.cos(t_n) + X_0[1, :] * np.cos(t_n))\n",
    "    y_1 = u * (X_0[0, :] * np.sin(t_n) + X_0[1, :] * np.cos(t_n))\n",
    "                 \n",
    "    X_1 = np.array([x_1, y_1])\n",
    "    \n",
    "    return X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def animate_enkf(B_var = 0.1, R_var = 0.1, N=2, ens_n=3):\n",
    "\n",
    "    # define the static background and observational error covariances\n",
    "    P_0 = B_var * np.eye(2)\n",
    "    R = R_var * np.eye(2)\n",
    "\n",
    "    # set a random seed for the reproducibility\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # we define the mean for the background\n",
    "    x_b = np.array([0,0])\n",
    "    \n",
    "    # and the initial condition of the real state as a random draw from the prior\n",
    "    x_t = np.random.multivariate_normal([0,0], P_0)\n",
    "\n",
    "    y_obs = np.zeros([2,N-1])\n",
    "    \n",
    "    # define the Ikeda map parameter\n",
    "    u = 0.9\n",
    "    for i in range(N-1):\n",
    "        # we forward propagate the true state\n",
    "        x_t = Ikeda(x_t, u)\n",
    "    \n",
    "        # and generate a noisy observation\n",
    "        y_obs[:, i] = x_t + np.random.multivariate_normal([0,0], R)\n",
    "    \n",
    "    \n",
    "    # we define the ensemble as a random draw of the prior\n",
    "    ens = np.random.multivariate_normal(x_b, P_0, size=ens_n).transpose()\n",
    "\n",
    "    \n",
    "    # define the Ikeda map parameter\n",
    "    for i in range(N-1):\n",
    "        \n",
    "        # forward propagate the last analysis\n",
    "        ens_f = Ikeda_V(ens, u)\n",
    "        \n",
    "        # we generate observation perturbations\n",
    "        obs_perts =  np.random.multivariate_normal([0,0], R, size=ens_n)\n",
    "        obs_perts = obs_perts - np.mean(obs_perts, axis=0)\n",
    "        \n",
    "        # we generate the ensemble based observation error covariance \n",
    "        obs_cov = obs_perts.transpose() @ obs_perts / (ens_n - 1)\n",
    "        \n",
    "        # we perturb the observations\n",
    "        perts_obs = np.squeeze(y_obs[:,i]) + obs_perts\n",
    "        \n",
    "        # we compute the ensemble mean and anomalies\n",
    "        X_mean_f = np.mean(ens_f, axis=1)\n",
    "        A_t = (ens_f.transpose() - X_mean_f) / np.sqrt(ens_n - 1)\n",
    "        \n",
    "        # and the ensemble covariances\n",
    "        P = A_t.transpose() @ A_t\n",
    "\n",
    "        # we compute the ensemble based gain and the analysis ensemble\n",
    "        K_gain = P @ np.linalg.inv( P + obs_cov)\n",
    "        ens = ens_f + K_gain @ (perts_obs.transpose() - ens_f)\n",
    "        X_mean_a = np.mean(ens, axis=1)\n",
    "\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.add_axes([.1, .1, .8, .8])\n",
    "    \n",
    "    l1 = ax.scatter(ens_f[0, :], ens_f[1, :], c='k', s=20, alpha=.5, marker=',')\n",
    "    ax.scatter(X_mean_f[0], X_mean_f[1], c='k', s=200, marker=\"D\")\n",
    "    \n",
    "    l3 = ax.scatter(ens[0, :], ens[1, :], c='b', s=20, alpha=.5, marker=',')\n",
    "    ax.scatter(X_mean_a[0], X_mean_a[1], c='b', s=200, marker=\"D\")\n",
    "    \n",
    "    l2 = ax.scatter(y_obs[0, -1], y_obs[1, -1], c='r', s=20)\n",
    "    ax.add_patch(Ellipse(y_obs[:,-1], R_var, R_var, ec='r', fc='none'))\n",
    "    \n",
    "    \n",
    "    ax.set_xlim([-2, 4])\n",
    "    ax.set_ylim([-4,2])\n",
    "    \n",
    "    labels = ['Forecast', 'Observation', 'Analysis']\n",
    "    plt.legend([l1,l2,l3],labels, loc='upper right', fontsize=26)\n",
    "    plt.show()\n",
    "    \n",
    "w = interactive(animate_enkf,B_var=(0.01,1.0,0.01), R_var=(0.01,1.0,0.01), N=(2, 50, 1), ens_n=(3,300, 3))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Summary of the EnKF</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">The EnKF makes a vast improvement over the earlier explored methods and demonstrates its robustness as a learning scheme <em>when there are sufficiently many samples.</em></li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">It should be noted that this requires vastly fewer samples than implementing an effective bootstrap particle filter, but at the cost of introducing bias in the analysis of the posterior.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\">However in operational settings, the reality is that ensemble-based forecasting is still highly expensive and most operational EnKF uses at most $\\mathcal{O}\\left(10^2\\right)$ samples in the learning.</li>\n",
    "    <li class=\"fragment\">While the EnKF is highly parallelizable, numerical weather prediciton models require massive computation power and this fundamentally limits the number of available samples.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Making the EnKF work in practice</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">The reality of implementing the EnKF in a numerical weather prediction setting is that the covariance estimates will also be highly biased and extremely rank deficient.</li>\n",
    "    <li class=\"fragment\">While there are reasons to belive that the true Bayesian posterior covariance is also rank deficient, in the end we especially rely on:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">inflation; and</li>\n",
    "        <li class=\"fragment\">localization;</li>\n",
    "    </ol>\n",
    "    <li class=\"fragment\">in order to relax the error estimates (introduce variance) and rectify the extreme rank deficiency.</li>\n",
    "    <li class=\"fragment\">Both of these techniques are highly active research areas for improving ensemble-based filtering, which are discussed in, e.g., the recent review article of Carrassi et al.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
