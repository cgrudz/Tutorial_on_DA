{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Mathematical Theory of Data Assimilation with Applications:<br>\n",
    "\n",
    "<p class=\"fragment\">Tutorial part 3 of 4 --- introducing flow dependence of the errors<p></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Letting the prior vary</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">A weakness of the earlier approach was that the prior did not accumulate new information over the observation-analysis-forecast cycle.</li>\n",
    "    <li class=\"fragment\">We can increase the accumulation of the information in the DA scheme by taking a recursive form for the learning.</li>\n",
    "    <li class=\"fragment\">This is again a strength of the Bayesian framework for DA, in which there is a natural recursive formulation:</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\">particularly, evolving the last posterior forward in time gives a natural choice for the subsequent prior.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\">We will consider how to formulate this in scalar, linear-Gaussian equation studied earlier.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The simple example</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">We consider once again the simple estimation of the true air temperature $T_t$ at the ground in Reading from:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">a background state $T_b$ generated via a numerical forecast model; and</li>\n",
    "        <li class=\"fragment\">an observed state $T_o$.</li>\n",
    "    </ol>\n",
    "    <li class=\"fragment\"> From the frequentist perspective, we had assumed that these are independent and\n",
    "        \\begin{align}\n",
    "        T_b &= T_t + \\epsilon_b & & \\epsilon_b \\sim N(0, \\sigma_b^2)\\\\\n",
    "        T_o &= T_t + \\epsilon_o & & \\epsilon_o \\sim N(0, \\sigma_o^2)\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> This is equivalent to re-writing the statement as,\n",
    "             \\begin{align}\n",
    "        T_t &\\sim N(T_b, \\sigma_b^2)\\\\\n",
    "        T_o &\\sim N(T_t, \\sigma_o^2)\n",
    "        \\end{align}\n",
    "        following a Bayesian perspective.\n",
    "    </li>   \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The simple example continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> We found that the minimum mean-square-error (minimum variance) analysis state can be derived by a linear combination of the two sources of information,\n",
    "    \\begin{align}\n",
    "     T_a = a_bT_b + a_o T_o\n",
    "     \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">These weights $a_b, a_o$ take the form,\n",
    "        \\begin{align}\n",
    "        a_b &= \\frac{a_o^2}{a_b^2 + a_o^2} & & a_o = \\frac{a_b^2}{a_b^2 + a_o^2}\n",
    "        \\end{align}\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The innovation</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> However, we will find it convenient to change this relationship in order to mirror the Bayesian update step.</li>\n",
    "    <li class=\"fragment\">Specifically, we treat the $T_b$ as a prior estimate that we would like to \"update\" with its weighted difference from the observation:</li>\n",
    "        <li class=\"fragment\">\n",
    "        \\begin{align}\n",
    "        T_a &= T_b - T_b + a_bT_b + a_o T_o \\\\\n",
    "            &= T_b + \\frac{T_b \\sigma_o^2 - T_b(\\sigma_b^2 + \\sigma_o^2) + \\sigma_b^2 T_o}{\\sigma_b^2 + \\sigma_o^2} \\\\\n",
    "            &= T_b + W\\left(T_o - T_b\\right)\n",
    "        \\end{align}\n",
    "        where $W = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}$.\n",
    "    </li>\n",
    "    <li class=\"fragment\">The value $T_o - T_b$ is called the observational \"innovation\".</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Deriving the analysis error variance</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> We will define the analysis error in the same way as we defined the error of the other states,\n",
    "    \\begin{align}\n",
    "    \\epsilon_a = T_a - T_t    & & T_t \\sim N(T_a, \\sigma_a^2)\n",
    "    \\end{align}\n",
    "     which is Gaussian distributed because it is the sum of Gaussian random variables.\n",
    "    </li>\n",
    "    <li class=\"fragment\"><b>Exercise (6 minutes):</b> use the statement\n",
    "        \\begin{align}\n",
    "        T_a = T_b + W\\left(T_o - T_b\\right)\n",
    "        \\end{align}</li>\n",
    "        <li class=\"fragment\">and the statement\n",
    "        \\begin{align}\n",
    "        W = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">to derive the analysis error variance $\\sigma_a^2$ as a function of the optimal weights $W$ and the background error variance $\\sigma_b^2$. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Deriving the analysis error variance continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"><b>Solution:</b> consider that,\n",
    "    \\begin{align}\n",
    "        \\epsilon_a &= T_a - T_t \\\\\n",
    "                   &= T_t + \\epsilon_b - T_t + W(T_t + \\epsilon_o - T_t - \\epsilon_b)\\\\\n",
    "                   &= \\epsilon_b + W(\\epsilon_o - \\epsilon_b)\\\\\n",
    "                   &= (1 - W)\\epsilon_b + W\\epsilon_o.\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">Therefore, we find that,\n",
    "        \\begin{align}\n",
    "        \\sigma_a^2 &= \\mathbb{E}[\\epsilon_a^2] \\\\\n",
    "        &=(1-W)^2 \\sigma_b^2 + W^2 \\sigma_o^2 \\\\\n",
    "        &= \\sigma_b^2 - 2 \\sigma_b^2 W  + W^2 \\left(\\sigma_b^2 +\\sigma_o^2\\right) \\\\\n",
    "        &= \\sigma_b^2 - 2 \\sigma_b^2 W  + \\sigma_b^2 W\\\\\n",
    "        &= (1 - W)\\sigma_b^2\n",
    "        \\end{align}\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The analysis error variance</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">We found that the posterior error variance can be computed recursively from the prior error variance as,\n",
    "    \\begin{align}\n",
    "    \\sigma_a^2 = (1 - W) \\sigma_b^2 & & W = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}.\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">By definition, $0 \\leq W\\leq 1$, so that $\\sigma_a^2 \\leq \\sigma_b^2$ and the assimilation step has the effect of reducing the variance of the background error.</li>\n",
    "    <li class=\"fragment\">With the analysis error variance in hand, we can <em>forecast the entire posterior</em> like the optimal analysis state earlier;</li>\n",
    "    <li class=\"fragment\">this forecasted posterior becomes the obvious choice for the next prior, if we have a reasonable way to compute it.</li>\n",
    "    <li class=\"fragment\">We will expand our consideration slightly to a more realistic model for this as well.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Re-introducing the forecast and observational models</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> Let's re-introduce a simple dynamical model and observational model,\n",
    "        \\begin{align}\n",
    "        \\mathbf{x}_{k} &= \\mathbf{M} \\mathbf{x}_{k-1} + \\mathbf{w}_k & & \\mathbf{M} \\in \\mathbb{R}^{n\\times n} & & \\mathbf{x}_k,\\mathbf{w}_k \\in \\mathbb{R}^n\\\\\n",
    "        \\mathbf{y}_{k} &= \\mathbf{H} \\mathbf{x}_k + \\mathbf{v}_k & & \\mathbf{H} \\in \\mathbb{R}^{d \\times n} & & \\mathbf{y}_k, \\mathbf{v}_k \\in \\mathbb{R}^{d} \n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> The new term $\\mathbf{w}_k$ will represent the assumption that our model is inherently stochastic, or that there are errors in our model of the physical process;\n",
    "    <li class=\"fragment\"> this may arise due to, e.g., physics on small scales that we cannot account for in the simple model $\\mathbf{M}$.</li>\n",
    "    <li class=\"fragment\"> While the actual errors will be unkown, we will assume that $\\mathbf{w}_k \\sim N(0,\\mathbf{Q})$ where $\\mathbf{Q}$ is some known matrix for the covariance.</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\"> This assumes implicitly that the model $\\mathbf{M}$ is unbiased;</li>\n",
    "        <li class=\"fragment\"> if this isn't the case, post-processing of the forecast for bias correction or model redesign may be necessary.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The forecast distribution</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> It can be shown that if</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\"> $\\mathbf{M}\\in\\mathbb{R}^{n \\times n}$ is a linear operator;</li>\n",
    "        <li class=\"fragment\"> $\\mathbf{w}_k\\sim N(0 ,\\mathbf{Q})$; and</li>\n",
    "        <li class=\"fragment\"> if the probability measure $\\mathbf{P}_{\\mathbf{x}_b, \\mathbf{B}}(\\mathbf{x}_{k-1})$ is Gaussian distributed;</li>\n",
    "    </ol>\n",
    "     <li class=\"fragment\"> then the forward evolution of states $\\mathbf{x}_{k} = \\mathbf{M}\\mathbf{x}_{k-1}+\\mathbf{w}_{k}$ will also have probability measure with Gaussian distribution.</li>\n",
    "    <li class=\"fragment\"> Gaussian distributions are entirely characterized by their first two moments, such that we can describe the forecast distribution by the forward-evolved mean and covariance.</li>\n",
    "    <li class=\"fragment\"> We will derive this in the simple example.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Describing the forecast mean</h3>\n",
    "<ul>\n",
    "        <li class=\"fragment\">In the Bayesian perspective, we will again treat the true temperature $T_t \\sim N\\left(T_b,\\mathbf{B}\\right)$ as a <em>random variable</em> where the background prior state is the mean.</li> \n",
    "    <li class=\"fragment\">  Recall that the minimum variance analysis state was defined by the <em>update to the background state</em> by\n",
    "    \\begin{align}\n",
    "        T_a = T_b + W\\left(T_o - T_b\\right)\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\"> This analysis state represents the mean (or expected value) for the unkown true temperature conditioned on the observation $T_o \\sim N(T_t, R)$.</li>\n",
    "    <ul>\n",
    "        <li class=\"fragment\"> Before the observation $T_t \\sim N(T_b, \\sigma^2_b)$, but $T_t \\sim N(T_a , \\sigma_a^2)$ once conditioning on $T_o$ according to the Bayesian update.</li>\n",
    "    </ul>\n",
    "    <li class=\"fragment\"> Noting that $0 \\leq W \\leq 1$, the recursion,\n",
    "            \\begin{align}\n",
    "            \\sigma_a^2 = (1 - W) \\sigma_b^2,\n",
    "            \\end{align}\n",
    "            implies that the variance of the distribution around the analysis mean is less than or equal to the variance of the distribution around the background.</li>\n",
    "        <li class=\"fragment\">Particularly, conditioning on the observation reduces the overall uncertainty of the true temperature $T_t \\sim N(T_a, \\sigma^2_a)$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Describing the forecast mean continued</h3>\n",
    "<ul>\n",
    "     <li class=\"fragment\"> The true temperature evolves under the forecast model\n",
    "        \\begin{align}\n",
    "        T_t(k+1) = \\mathbf{M} T_t(k) + \\mathbf{w}_{k+1}\n",
    "        \\end{align}\n",
    "        </li>\n",
    "    <li class=\"fragment\"> <b>Q:</b> what should the mean of the forward-evolved state look like in this example?</li>\n",
    "    <li class=\"fragment\"><b>A:</b> the mean for this state is derived as,\n",
    "            \\begin{align}\n",
    "           T_b(k+1) &\\triangleq \\mathbb{E}\\left[T_t(k+1)\\right] \\\\\n",
    "        &= \\mathbb{E}\\left[ \\mathbf{M}T_t(k) + \\mathbf{w}_{k+1}\\right] \\\\\n",
    "        &= \\mathbb{E}\\left[ \\mathbf{M}T_t(k)\\right] + \\mathbb{E}\\left[\\mathbf{w}_{k+1}\\right] \\\\\n",
    "            &=\\mathbf{M}T_a(k) \n",
    "            \\end{align}</li>\n",
    "    <li class=\"fragment\">That is, the mean of the new prior is derived exactly as the deterministic evolution of the analysis mean.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Describing the forecast (co)-variance</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">  In the simple example, the forecast error can thus be derived directly as,\n",
    "    \\begin{align}\n",
    "    \\epsilon_{b}(k+1)& = T_t(k+1) - T_b(k+1) \\\\\n",
    "        &= \\mathbf{M}T_t(k) + \\mathbf{w}_{k+1} - \\mathbf{M}T_a (k) \\\\\n",
    "        &= \\mathbf{M}\\left( T_t(k) - T_a(k)\\right) + \\mathbf{w}_{k+1}\\\\\n",
    "        &= \\mathbf{M}\\left( \\epsilon_a(k) \\right) + \\mathbf{w}_{k+1}\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">In the simple scalar example, we thus recover\n",
    "        \\begin{align}\n",
    "        \\sigma_b^2(k+1) &= \\mathbb{E}\\left[\\left(\\mathbf{M}\\left( \\epsilon_a(k) \\right) + \\mathbf{w}_{k+1}\\right)^2\\right]\\\\\n",
    "        & = \\mathbf{M} \\sigma_a^2(k)\\mathbf{M} + \\mathbf{Q}\n",
    "        \\end{align}</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The observation-analysis-forecast cycle</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">The two-step process we have now derived describes the evolution of the posterior into the next prior at all times.</li>\n",
    "    <li class=\"fragment\">Using the analysis update step, we can define the next posterior whenever new information comes in the form of an observation.</li>\n",
    "    <li class=\"fragment\">This new posterior is then evolved forward in time by the numerical model (with errors) to define the <b>next prior</b>.</li>\n",
    "    <li class=\"fragment\">The cycle can continue ad infinitum;</li>\n",
    "    <li class=\"fragment\">this simple example explains the basis of the <em>Kalman filter</em>, which we have derived in one dimension.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Kalman filter in multiple dimensions</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">The recursion we described in terms of the one dimensional posterior extends to multiple dimensions as follows:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\"> suppose at time $t_0$ the model prior state is distributed $\\mathbf{x}_0 \\sim N\\left(\\overline{\\mathbf{x}}_0, \\mathbf{P}_0\\right) $;</li>\n",
    "        <li class=\"fragment\"> suppose at each time $t_k$ for $k=1,\\cdots$ the dynamical and observational models are given,    \n",
    "          \\begin{align}\n",
    "        \\mathbf{x}_{k} &= \\mathbf{M} \\mathbf{x}_{k-1} + \\mathbf{w}_k & & \\mathbf{M} \\in \\mathbb{R}^{n\\times n} & & \\mathbf{x}_k,\\mathbf{w}_k \\in \\mathbb{R}^n & & \\mathbf{w}_k \\sim N(0, \\mathbf{Q})\\\\\n",
    "        \\mathbf{y}_{k} &= \\mathbf{H} \\mathbf{x}_k + \\mathbf{v}_k & & \\mathbf{H} \\in \\mathbb{R}^{d \\times n} & & \\mathbf{y}_k, \\mathbf{v}_k \\in \\mathbb{R}^{d} & & \\mathbf{v}_k \\sim N(0, \\mathbf{R}) \n",
    "        \\end{align}\n",
    "        </li>\n",
    "        <li class=\"fragment\">Then, the model forecast  $\\mathbf{x}_k \\sim N\\left(\\overline{\\mathbf{x}}_k^f, \\mathbf{P}^f_k\\right)$, where\n",
    "            \\begin{align}\n",
    "            \\overline{\\mathbf{x}}^f_k = \\mathbf{M} \\overline{\\mathbf{x}}^a_{k-1} & & \\mathbf{P}_k^f = \\mathbf{M} \\mathbf{P}_{k-1} \\mathbf{M}^\\mathrm{T} + \\mathbf{Q}.\n",
    "            \\end{align}\n",
    "        <li class=\"fragment\"> Conditioned on the observation $\\mathbf{y}_k$, the posterior for $\\mathbf{x}_k$ is given by $N\\left(\\overline{\\mathbf{x}}^a_k, \\mathbf{P}^a_k\\right)$ where,\n",
    "            \\begin{align}\n",
    "            \\overline{\\mathbf{x}}_k^a = \\overline{\\mathbf{x}}_k^f + \\mathbf{K}_k\\left(\\mathbf{y}_k - \\mathbf{H} \\overline{\\mathbf{x}}_k^f\\right) & & \\mathbf{P}_k^a = \\left(\\mathbf{I} - \\mathbf{K}_k\\mathbf{H}\\right) \\mathbf{P}_k^f\n",
    "            \\end{align}\n",
    "    </ol>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The Kalman filter in multiple dimensions continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> The optimal weights $\\mathbf{W}$ derived earlier are given in terms of the <em>Kalman gain</em> matrix, defined,\n",
    "    \\begin{align}\n",
    "        \\mathbf{K}_k &\\triangleq \\mathbf{P}_k^f \\mathbf{H}^\\mathrm{T}\\left( \\mathbf{R} + \\mathbf{H} \\mathbf{P}_k^f \\mathbf{H}^\\mathrm{T}\\right)^{-1}.\n",
    "    \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">Once again, the update solution is a combination of the background and the observation, weighted inverse-proportionately to their uncertainties.</li>\n",
    "    <li class=\"fragment\">However, in this case, the background uncertainty <em>varies in time</em> as the forecast of the last posterior covariance.</li> \n",
    "        <li class=\"fragment\">It can be shown like earlier that $\\mathbf{K}_k$ is a multi-dimensional form for combining the model forecast mean and the observation to construct the minimum mean-square-error (minimum variance) analysis state.</li>\n",
    "    <li class=\"fragment\">We will discuss this in the following.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Best Linear Unbiased Estimation (BLUE) </h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> The Kalman gain can be derived in the framework of the Gauss-Markov theorem, which provides the \"BLUE\".</li>\n",
    "    <li class=\"fragment\"> We suppose we have two time series of vectors,\n",
    "        \\begin{align}\n",
    "        \\mathbf{x}(t) = \\begin{pmatrix}x_1(t)&\\cdots & x_n(t)\\end{pmatrix}^\\mathrm{T}; & & \\mathbf{y}(t) = \\begin{pmatrix}y_1(t)&\\cdots & y_n(t)\\end{pmatrix}^\\mathrm{T};\n",
    "        \\end{align}\n",
    "        where each has been re-centered at zero by subtracting their respective means.</li>\n",
    "    <li class=\"fragment\">This is to say that $\\mathbb{E}[\\mathbf{x}] = \\mathbb{E}[\\mathbf{y}] = 0 $, i.e., these are vectors of <em>anomalies</em>.</li>\n",
    " </ul>   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Best Linear Unbiased Estimation (BLUE) continued</h3>\n",
    "<ul>\n",
    "<li class=\"fragment\">We will assume that there is some <em>linear relationship</em> between $\\mathbf{x}$ and $\\mathbf{y}$ that is represented by,\n",
    "        \\begin{align}\n",
    "        \\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\boldsymbol{\\epsilon}\n",
    "        \\end{align}</li>\n",
    "    <li class=\"fragment\"> As a multiple regression, we will write the estimated value for this relationship by,\n",
    "        \\begin{align}\n",
    "        \\mathbf{y}_a = \\hat{\\mathbf{W}} \\mathbf{x}\n",
    "        \\end{align}</li>\n",
    "    <li class=\"fragment\">\n",
    "        such that \n",
    "        \\begin{align}\n",
    "        \\mathbf{y} - \\mathbf{y}_a &= \\mathbf{y} - \\hat{\\mathbf{W}} \\mathbf{x} \\\\\n",
    "                                  &= \\hat{\\boldsymbol{\\epsilon}}\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">The Gauss-Markov theorem loosely states that the weights $\\hat{\\mathbf{W}}$ found by least-squares, i.e., minimizing the expected residual sum of squares\n",
    "    \\begin{align}\n",
    "    RSS =  \\hat{\\boldsymbol{\\epsilon}}^\\mathrm{T} \\hat{\\boldsymbol{\\epsilon}},\n",
    "    \\end{align}\n",
    "    is the best-linear-unbiased-estimator of the true relationship $\\mathbf{W}$.</li>\n",
    "    <li class=\"fragment\">\"Best\" in the sense of the Gauss-Markov theorem is to say that the weights $\\hat{\\mathbf{W}}$ will be the minimum-variance estimate, as compared with other unbiased estimates of the true relationship $\\mathbf{W}$.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Best Linear Unbiased Estimation (BLUE) continued</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\"> To find the minimizing $\\hat{\\mathbf{W}}$, we differentiate the expected RSS, i.e.,\n",
    "        \\begin{align}\n",
    "        \\frac{\\partial}{\\partial W_{ij}} \\mathbb{E}\\left[ \\hat{\\boldsymbol{\\epsilon}}^\\mathrm{T} \\hat{\\boldsymbol{\\epsilon}}\\right] & = \\mathbb{E}\\left[ \\left\\{\\mathbf{W} \\mathbf{y}\\mathbf{y}^\\mathrm{T}\\right\\}_{ij} - \\left\\{\\mathbf{x}\\mathbf{y}^\\mathrm{T}\\right\\}_{ij} \\right]\n",
    "        \\end{align}\n",
    "<li class=\"fragment\">Setting the equation to zero for some choice of $\\hat{\\mathbf{W}}$, we obtain the normal equation\n",
    "    \\begin{align}\n",
    "     & &\\hat{\\mathbf{W}}\\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^\\mathrm{T}\\right] - \\mathbb{E}\\left[ \\mathbf{x}\\mathbf{y}^\\mathrm{T}\\right]&= 0\\\\\n",
    "    \\Leftrightarrow & & \\hat{\\mathbf{W}}=  \\mathbb{E}\\left[\\mathbf{x}\\mathbf{y}^\\mathrm{T}\\right] \\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^\\mathrm{T}\\right]^{-1}   \n",
    "    \\end{align}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>BLUE in data assimlation</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">With the background and analysis errors defined as before,\n",
    "        \\begin{align}\n",
    "        \\boldsymbol{\\epsilon}_b= \\mathbf{x}_b - \\mathbf{x}_t ; & &\n",
    "        \\boldsymbol{\\epsilon}_a= \\mathbf{x}_a - \\mathbf{x}_t;\n",
    "        \\end{align}\n",
    "    </li>\n",
    "    <li class=\"fragment\">we also define the observation error as,\n",
    "        \\begin{align}\n",
    "        \\boldsymbol{\\epsilon} &= \\mathbf{y} - \\mathbf{H}\\mathbf{x}_t,\n",
    "        \\end{align}\n",
    "        where $\\mathbf{H}$ is the observation operator.</li>\n",
    "    <li class=\"fragment\">Using the above, we re-write the observational innovation as,\n",
    "        \\begin{align}\n",
    "        \\boldsymbol{\\delta} &= \\mathbf{y} - \\mathbf{H}\\mathbf{x}_b = \\mathbf{y} - \\mathbf{H}\\left[\\mathbf{x}_t + \\left(\\mathbf{x}_b - \\mathbf{x}_t\\right)\\right] \\\\\n",
    "        & = \\mathbf{y} - \\mathbf{H}\\mathbf{x}_t  - \\mathbf{H}\\left(\\mathbf{x}_b - \\mathbf{x}_t\\right)= \\boldsymbol{\\epsilon}_o - \\mathbf{H} \\boldsymbol{\\epsilon}_b\n",
    "        \\end{align}</li>\n",
    "  <ul>      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>BLUE in data assimlation continued</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">We will require that the analysis state estimate is once again unbiased;</li>\n",
    "    <li class=\"fragment\">recall that $\\overline{\\mathbf{x}}_a = \\overline{\\mathbf{x}}_b + \\mathbf{W}\\delta$ so that we can obtain\n",
    "        \\begin{align}\n",
    "        &\\overline{\\mathbf{x}}_a = \\mathbf{x}_t + \\boldsymbol{\\epsilon}_a \\\\\n",
    "        \\Leftrightarrow & \\mathbf{x}_t - \\overline{\\mathbf{x}}_b = \\mathbf{W}\\delta -\\boldsymbol{\\epsilon}_a\\\\\n",
    "        \\Leftrightarrow & \\boldsymbol{\\epsilon}_b = \\mathbf{W}\\boldsymbol{\\delta} - \\boldsymbol{\\epsilon}_a\n",
    "        \\end{align}\n",
    "        </ul>\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "<h3>BLUE in data assimlation continued</h3>\n",
    "<ul>\n",
    "<li class=\"fragment\">Assuming that the background and observation errors are uncorrelated, the choice of $\\hat{\\mathbf{W}}$ that minimizes $\\boldsymbol{\\epsilon}^\\mathrm{T}_a \\boldsymbol{\\epsilon}_a$ is given as\n",
    "            \\begin{align}\n",
    "            \\hat{\\mathbf{W}} &= \\mathbb{E}\\left[\\left(-\\boldsymbol{\\epsilon}_b\\right)\\left(-\\boldsymbol{\\delta}\\right)^\\mathrm{T}\\right]\\mathbb{E}\\left[\\left(-\\boldsymbol{\\delta}\\right) \\left(-\\boldsymbol{\\delta}\\right)^\\mathrm{T}\\right]^{-1}\\\\\n",
    "            &= \\mathbb{E}\\left[ \\left(-\\boldsymbol{\\epsilon}_b\\right)\\left(\\boldsymbol{\\epsilon}_o - \\mathbf{H}\\boldsymbol{\\epsilon}_b\\right)^\\mathrm{T}\\right]\\mathbb{E}\\left[\\left(\\boldsymbol{\\epsilon}_o - \\mathbf{H}\\boldsymbol{\\epsilon}_b\\right) \\left(\\boldsymbol{\\epsilon}_o - \\mathbf{H}\\boldsymbol{\\epsilon}_b\\right)^\\mathrm{T}\\right]^{-1} \\\\\n",
    "            &= \\mathbf{B}\\mathbf{H}\\left(\\mathbf{H}\\mathbf{B}\\mathbf{H}^\\mathrm{T}+\\mathbf{R}\\right)^{-1}\n",
    "            \\end{align}</li>\n",
    "    <li class=\"fragment\">With the above derivation, we find that the mean of the posterior (the BLUE estimate) is given by the update of the Kalman gain recursion.</li> \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>The extended Kalman filter</h3>\n",
    "<ul>\n",
    "    <li class=\"fragment\">The Kalman filter provides a parametric recursion for the Bayesian posterior when the dynamic and observation models are linear, and all error distributions are Gaussian.</li> \n",
    "    <li class=\"fragment\"> In most cases, however, the numerical model will not be linear and so $\\mathbf{M}$ will represent the <em>linearized</em> numerical model along some nonlinear trajctory.</li>\n",
    "        <li class=\"fragment\">The process of:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">evolving the estimated mean state with the fully nonlinear model; </li>\n",
    "        <li class=\"fragment\">while approximating the evolution of the covariance with the linearized equations about this trajectory; and</li>\n",
    "        <li class=\"fragment\">linearizing the relationship between the model variables and the observations;</li>\n",
    "    </ol>\n",
    "    <li class=\"fragment\">is known as <em>extended Kalman filtering</em>.</li>\n",
    "    <li class=\"fragment\">We will demonstrate this technique in the Ikeda model.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> The extended Kalman filter continued</h3>\n",
    "\n",
    "<ul>\n",
    "  <li class=\"fragment\"> In the following slide, we will attempt to extended Kalman filter in the Ikeda map.</li>\n",
    "  <li class=\"fragment\"> The code chunk below defines the Jacobian of the map, used to propagate the covariance in in the forecast step.</li>\n",
    "  <li class=\"fragment\"><b>Exercise (2 minutes):</b> use the sliders in the following slide to examine how the covariance changes due to the flow dependence.  Then consider the following questions:</li>\n",
    "    <ol>\n",
    "            <li class=\"fragment\">How does the analysis covariance differ from the fixed background prior?</li>\n",
    "            <li class=\"fragment\">How does the analysis covariance change with respect to the forecast covariance at each step?</li>\n",
    "            <li class=\"fragment\">How does the analysis covariance change with respect to different observation error variances?</li>\n",
    "    </ol>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     20
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "\n",
    "def Ikeda(X_0, u):\n",
    "    \"\"\"The array X_0 will define the initial condition and the parameter u controls the chaos of the map\n",
    "    \n",
    "    This should return X_1 as the forward state.\"\"\"\n",
    "    \n",
    "    t_1 = 0.4 - 6 / (1 + X_0.dot(X_0) )\n",
    "    \n",
    "    x_1 = 1 + u * (X_0[0] * np.cos(t_1) + X_0[1] * np.cos(t_1))\n",
    "    y_1 = u * (X_0[0] * np.sin(t_1) + X_0[1] * np.cos(t_1))\n",
    "                 \n",
    "    X_1 = np.array([x_1, y_1])\n",
    "    \n",
    "    return X_1\n",
    "\n",
    "def Ikeda_jacobian(X_0, u):\n",
    "    \n",
    "    # define the partial derviative of t with respect to v\n",
    "    def dt_dv(v,w):\n",
    "        return 12 * v / ( (1 + w**2 + v**2) ** (2) )\n",
    "\n",
    "    # unpack the values for x and y\n",
    "    x, y = X_0\n",
    "    \n",
    "    # compute t\n",
    "    t = 0.4 - 6 / (1 + x**2 + y**2)\n",
    "    \n",
    "    # evaluate the partial derivatives\n",
    "    df1_dx = u * (np.cos(t) - x * np.sin(t) * dt_dv(x,y) + y * np.cos(t) * dt_dv(x,y))\n",
    "    df1_dy = u * (-x * np.sin(t) * dt_dv(y,x) + np.sin(t) + y * np.cos(t) * dt_dv(y,x))\n",
    "    df2_dx = u * (np.sin(t) + x * np.cos(t) * dt_dv(x,y) - y * np.sin(t) * dt_dv(x,y))\n",
    "    df2_dy = u * (x * np.cos(t) * dt_dv(y,x) + np.cos(t) - y * np.sin(t) * dt_dv(y,x))\n",
    "    \n",
    "    return np.array([[df1_dx, df1_dy], [df2_dx, df2_dy]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     17
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def animate_ext_kf(B_var = 0.1, R_var = 0.1, k=2):\n",
    "\n",
    "    # define the static background and observational error covariances\n",
    "    P_0 = B_var * np.eye(2)\n",
    "    R = R_var * np.eye(2)\n",
    "\n",
    "    # set a random seed for the reproducibility\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # we define the mean for the background\n",
    "    x_b = np.array([0,0])\n",
    "    \n",
    "    # and the initial condition of the real state as a random draw from the prior\n",
    "    x_t = np.random.multivariate_normal([0,0], P_0)\n",
    "\n",
    "    # define the Ikeda map parameter\n",
    "    u = 0.75\n",
    "    for i in range(k-1):\n",
    "        \n",
    "        # we forward propagate the true state\n",
    "        x_t = Ikeda(x_t, u)\n",
    "        \n",
    "        # and generate a noisy observation\n",
    "        y_obs = x_t + np.random.multivariate_normal([0,0], R)\n",
    "        \n",
    "        # forward propagate the last analysis\n",
    "        x_b_f = Ikeda(x_b, u)\n",
    "        \n",
    "        # forward propagate the covariance\n",
    "        J = Ikeda_jacobian(x_b, u)\n",
    "        P_1 = J @ P_0 @ J.transpose()\n",
    "        \n",
    "        # analyze the observation\n",
    "        K = P_1 @ np.linalg.inv(P_1 + R)\n",
    "        x_b = x_b_f + K @ (y_obs - x_b_f)\n",
    "        P_0 = (np.eye(2) - K) @ P_1\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.add_axes([.1, .1, .8, .8])\n",
    "    \n",
    "    l1 = ax.scatter(x_b_f[0], x_b_f[1], c='k', s=40)\n",
    "    w, v = np.linalg.eigh(P_1)\n",
    "    ANGLE = np.pi / 2 - np.arctan(v[0][0]/ v[0][1])\n",
    "    ANGLE = ANGLE * 180 / np.pi\n",
    "    ax.add_patch(Ellipse(x_b_f, w[0], w[1], angle=ANGLE, ec='k', fc='none'))\n",
    "    \n",
    "    l2 = ax.scatter(y_obs[0], y_obs[1], c='r', s=40)\n",
    "    ax.add_patch(Ellipse(y_obs, R_var, R_var, ec='r', fc='none'))\n",
    "    \n",
    "    \n",
    "    l3 = ax.scatter(x_b[0], x_b[1], c='b', s=40)\n",
    "    w, v = np.linalg.eigh(P_0)\n",
    "    ANGLE = np.pi / 2 - np.arctan(v[0][0]/ v[0][1])\n",
    "    ANGLE = ANGLE * 180 / np.pi\n",
    "    ax.add_patch(Ellipse(x_b, w[0], w[1], angle=ANGLE, ec='b', fc='none'))\n",
    "    \n",
    "    \n",
    "    ax.set_xlim([-1, 4])\n",
    "    ax.set_ylim([-3,1])\n",
    "    \n",
    "    labels = ['Forecast', 'Observation', 'Analysis']\n",
    "    plt.legend([l1,l2,l3],labels, loc='upper right', fontsize=26)\n",
    "    plt.show()\n",
    "    \n",
    "w = interactive(animate_ext_kf,B_var=(0.01,1.0,0.01), R_var=(0.001,1.0,0.001), k=(2, 50, 1))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Issues with extended Kalman filters</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\">Although the extended Kalman filter is able to introduce flow dependence in the forecasted prior, it suffers from several issues:</li>\n",
    "    <ol>\n",
    "        <li class=\"fragment\">the linear assumption that is enforced in the evolution of the posterior is extremely  unrealistic, especially for the Ikeda map (which is highly nonlinear);</li>\n",
    "        <li class=\"fragment\">moreover, computing the Jacobian and the tangent-linear evolution is very computationally heavy and is not feasible for operational models;</li>\n",
    "        <li class=\"fragment\">finally, using entirely a parametric form for the evolution leads to catastrophic divergence in the estimates when the parametric equations are not well satisfied, see e.g., <a href=\"https://journals.ametsoc.org/doi/abs/10.1175/1520-0469%281994%29051%3C1037%3AADAISN%3E2.0.CO%3B2\" target=\"blank\">one classic paper on filter divergence and the extended Kalman filter</a>.</li>\n",
    "    </ol>\n",
    "    <li class=\"fragment\">Particularly in the previous example, we saw how the over-confidence of the extended Kalman filter covariance (being flat in one direction) meant that it wasn't receptive to observations which had almost no uncertainty.</li>\n",
    "    </ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Issues with extended Kalman filters continued</h3>\n",
    "\n",
    "<ul>    \n",
    "    <li class=\"fragment\"> Catastrophic divergence occurs when the extended Kalman filter no longer tracks the observations and the computation itself become singular;</li> \n",
    "    <li class=\"fragment\"> this is particularly problematic because it wouldn't be solved with infinite computational resources.</li>\n",
    "    <li class=\"fragment\"> Indeed, a purely parametric approach to computing the (non-Gaussian) posterior via the tangent-linear evolution of the second moment is too rigid to handle severe nonlinearities.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Sampling</h3>\n",
    "\n",
    "<ul>\n",
    "    <li class=\"fragment\"> For these reasons, we can consider following a more fully Bayesian analysis, by sampling the posterior and forecasting the samples to estimate the next prior directly.</li>\n",
    "    <li class=\"fragment\">This also allows us to sample from highly non-Gaussian distributions, possibly eliminating this unrealistic assumption.</li>\n",
    "    <li class=\"fragment\"> This philosophy is the basis of the particle filter and ensemble Kalman filter, which we will discuss next.</li>\n",
    "    <li class=\"fragment\"> Each of these represents a more direct Bayesian approach to the data assimilation problem; \n",
    "        <ul>\n",
    "        <li class=\"fragment\"> a central difference, however, will be in how each learning scheme handles the bias/ variance tradeoff in estimating the true relationship.</li>\n",
    "        </ul>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
